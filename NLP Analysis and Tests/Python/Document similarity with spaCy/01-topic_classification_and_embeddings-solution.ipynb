{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class tasks: the case of topic classification\n",
    "\n",
    "In one of the previous modules you learned about binary classification and how to implement it using machine learning.\n",
    "However, some classification tasks involve more than just two classes.\n",
    "In this module, you will learn how to address such tasks specifically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic classification\n",
    "\n",
    "Consider a program that collects articles from different websites.\n",
    "The articles span four broad subjects: _politics_, _science_, _tech_, and _sports cars_.\n",
    "When you open the news feed, all the articles are mixed together.\n",
    "What if you want to sort them by topic?\n",
    "\n",
    "Some RSS feed managers, such as Google News, implement this idea.\n",
    "\n",
    "![Google News with topic classification](figures/google_news_topics.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20 Newsgroups dataset\n",
    "\n",
    "In this module, you will use the [20 Newsgroups](http://qwone.com/~jason/20Newsgroups/) dataset, a collection of approximately 20,000 newsgroup documents.\n",
    "It is widely used by the NLP community to develop and test topic classification models, as well as text classification and text clustering algorithms.\n",
    "\n",
    "The data comes from 20 different newsgroups with different topics – which can be grouped into broad topic categories.\n",
    "In order to make the task more approachable, – you are still learning after all, – you will focus on four topics from four newsgroups:\n",
    "\n",
    "- **politics** from `talk.politics.misc`,\n",
    "- **electronics** from `sci.electronics`,\n",
    "- **Macintosh hardware** from `comp.sys.mac.hardware`, and\n",
    "- **automobiles** from `rec.autos`.\n",
    "\n",
    "Afterwards, when you feel comfortable with the implementation of the classifier, you can experiment with a different selection of topics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier\n",
    "\n",
    "Topic classification is more complicated than sentiment analysis: many words can appear in multiple topics – e.g., “hardware” can appear in articles about either _Macintosh hardware_ or _electronics_.\n",
    "Thus, the frequency of words and the probability of their occurrence in texts on different topics are a the most predictive type of feature.\n",
    "You will use it in your classifier implementation along with the [multinomial naïve Bayes](http://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes) model.\n",
    "\n",
    "Internally, the algorithm you will implement represents each topic (class) $y$ as a vector $\\theta$ of size $n$, where $n$ is the number of words taken into account for classification, the _vocabulary size_.\n",
    "The vectors are composed of $n$ estimates $\\theta_{y_i}$ of the probabilities $P(x_i|y)$ that the feature $i$ appears in texts on the topic $y$.\n",
    "These estimates can be computed from the data by counting the occurrences ($N_{y_i}$) of a specific feature ($i$) in training texts on a specific topic ($y$).\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta_{y_i} = \\frac{ N_{y_i} }{ N_y }\n",
    "\\end{equation}\n",
    "\n",
    "The number $N_{y_i}$ is calculated as the sum across texts on the topic $y$ of the uses of the word $i$.\n",
    "The number $N_y$ is the total count of all features for the class $y$: the sum $\\sum_{i=1}^{|T|} N_{y_i}$, where $T$ is the total number of topics.\n",
    "\n",
    "### Example\n",
    "\n",
    "Consider a simple vocabulary of just five words: “computer”, “motor”, “show”, “system”, and “visit”.\n",
    "In this simple example, all articles only consist of these five words – or, to be more realistic, you only care about these five words.\n",
    "\n",
    "Topic vectors (the $\\theta$s) have five elements (dimensions), one for each of the words in the vocabulary.\n",
    "You can expect the word “system” to appear in texts on _politics_ (e.g., “political system”) as well as on _Macintosh hardware_ (e.g., “operating system”).\n",
    "However, their frequency and the frequency of other words will vary.\n",
    "You can expect vectors such as:\n",
    "\n",
    "\n",
    " $\\theta_{\\mathit{topic}_\\mathit{feature}}$|    computer   |   motor   |   show   |   system   |   visit\n",
    "-------------------------------------------|:-------------:|:---------:|:--------:|:----------:|:---------\n",
    "**politics**                               |       0.0     |     0.0   |   0.1    |   0.4      |   0.5  \n",
    "**electronics**                            |       0.4     |     0.0   |   0.2    |   0.4      |   0.0\n",
    "**Macintosh hardware**                     |       0.5     |     0.0   |   0.1    |   0.4      |   0.0\n",
    "**automobiles**                            |       0.1     |     0.5   |   0.2    |   0.1      |   0.1  \n",
    "\n",
    "\n",
    "### Smoothing\n",
    "\n",
    "The table above contains null probabilities for the chance of some words occurring in documents on some topics (e.g., \"computer\" in _politics_).\n",
    "This can happen if, during the algorithm's training, the word “computer” never appears in articles about politics.\n",
    "\n",
    "However, human language is very diverse. And it is impossible to collect reliable statistics on the use of **all** words in **all** topics.\n",
    "As a result, in practice, you should avoid assigning null probabilities.\n",
    "This is why the formula is generally adjusted with a term $\\alpha$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta_{y_i} = \\frac{ N_{y_i} + \\alpha}{N_y + \\alpha \\times n}\n",
    "\\end{equation}\n",
    "\n",
    "Using $\\alpha = 1$ means that you increment all word counts by one; the count of unseen words becomes 1.\n",
    "In the formula above, $n$ is the total number of features, or the size of the vocabulary.\n",
    "Since you increment the counts for each of the $n$ features by $\\alpha$ in numerator, adding $\\alpha \\times n$ to denominator makes sure you still get\n",
    "proper probability distribution (i.e., $\\theta_{y_i}$ is still within the range of $[0, 1]$).\n",
    "This adjustment is called _Laplace smoothing_ or _add-one smoothing_.\n",
    "\n",
    "After smoothing, the table above becomes:\n",
    "\n",
    " $\\theta_{\\mathit{topic}_\\mathit{feature}}$|    computer   |   motor   |   show   |   system   |   visit\n",
    "-------------------------------------------|:-------------:|:---------:|:--------:|:----------:|:---------\n",
    "**politics**                               |      0.05     |    0.05   |  0.07    |   0.37     |   0.46  \n",
    "**electronics**                            |      0.37     |    0.05   |  0.16    |   0.37     |   0.05\n",
    "**Macintosh hardware**                     |      0.46     |    0.05   |  0.07    |   0.37     |   0.05\n",
    "**automobiles**                            |      0.07     |    0.46   |  0.16    |   0.07     |   0.07  \n",
    "\n",
    "Note that the order of the word frequencies is the same before and after smoothing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation with `sklearn`\n",
    "\n",
    "You will use [`sklearn`](http://scikit-learn.org/stable/index.html) for the implementation of the multinomial naïve Bayes classifier.\n",
    "\n",
    "* The `sklearn` library is a useful toolkit for data mining and data analysis. It is useful to add it to your belt.\n",
    "* It includes a range of classification techniques and data processing tools.\n",
    "* It has exactly the right features for analysing the 20 Newsgroups dataset.\n",
    "\n",
    "### Importing the libraries\n",
    "\n",
    "Start by importing the necessary libraries with the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several remarks are in order.\n",
    "First, note that `sklearn` includes direct support for the 20 Newsgroups dataset.\n",
    "Second, remember that the `TfidfVectorizer` import is for _Term Frequency times Inverse Document Frequency_ functionality.\n",
    "Third, the module `numpy` is imported `as` `np` which means that the identifier `np` in the rest of the code refers to `numpy`. This is just for convenience: to make code shorter.\n",
    "\n",
    "### Step 1: Loading the data\n",
    "\n",
    "The next step is to access and load the dataset.\n",
    "The `fetch_20newsgroups` function imported earlier helps you access the full dataset.\n",
    "\n",
    "Write a function `load_dataset` that wraps calls to `fetch_20newsgroups` and passes specific arguments:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(sset, cats):\n",
    "    if cats==[]:\n",
    "        newsgroups_dset = fetch_20newsgroups(subset=sset,\n",
    "                          remove=('headers', 'footers', 'quotes'),\n",
    "                          shuffle=True)\n",
    "    else:\n",
    "        newsgroups_dset = fetch_20newsgroups(subset=sset, categories=cats,\n",
    "                          remove=('headers', 'footers', 'quotes'),\n",
    "                          shuffle=True)\n",
    "    return newsgroups_dset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The argument `remove` simplifies the data by removing specific parts (here, the headers, footers and quote blocks).\n",
    "\n",
    "Use the function in your program: set categories and pass them to the `load_dataset` function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['talk.politics.misc', 'sci.electronics', 'comp.sys.mac.hardware', 'rec.autos']\n",
    "newsgroups_train = load_dataset('train', categories)\n",
    "newsgroups_test = load_dataset('test', categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(newsgroups_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Inspecting the data\n",
    "\n",
    "Once again, you should check that the data loaded correctly and you should get familiar with its format.\n",
    "Create a function `inspect_data` which prints out informative properties of the dataset\n",
    "\n",
    "* the target names\n",
    "* the shape of filenames\n",
    "* the shape of target\n",
    "* what target looks like\n",
    "* ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(newsgroups_train.target) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add your code here\n",
    "\n",
    "###https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html\n",
    "\n",
    "def inspect_data(dataset):\n",
    "    print(list(dataset.target_names))\n",
    "    print(dataset.filenames.shape)\n",
    "    print(dataset.target.shape)\n",
    "    print(dataset.target[:10])\n",
    "    \n",
    "newsgroups_test.target\n",
    "type(newsgroups_test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different elements that are printed are the different constituents of the dataset, as loaded by `sklearn`. You can check online the documentation for [the dataset](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html) or you can use the Python interpreter to explore it interactively.\n",
    "\n",
    "Call the function and read the output. Note that labels are represented as numerical values (`0` for `talk.politics.misc`, `1` for `sci.electronics`, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comp.sys.mac.hardware', 'rec.autos', 'sci.electronics', 'talk.politics.misc']\n",
      "(2228,)\n",
      "(2228,)\n",
      "[0 3 0 1 3 3 1 3 2 1]\n",
      "['comp.sys.mac.hardware', 'rec.autos', 'sci.electronics', 'talk.politics.misc']\n",
      "(1484,)\n",
      "(1484,)\n",
      "[3 1 0 1 1 0 0 2 0 2]\n"
     ]
    }
   ],
   "source": [
    "# add your code here\n",
    "#categories = ['talk.politics.misc', 'sci.electronics', 'comp.sys.mac.hardware', 'rec.autos']\n",
    "\n",
    "######## BETTER --- to sort the categories as it will be stored internally in an alphabetical order anyway!\n",
    "\n",
    "categories = sorted(['talk.politics.misc', 'sci.electronics', 'comp.sys.mac.hardware', 'rec.autos'])\n",
    "\n",
    "\n",
    "newsgroups_train = load_dataset('train', categories)\n",
    "inspect_data(newsgroups_train)\n",
    "newsgroups_test = load_dataset('test', categories)\n",
    "inspect_data(newsgroups_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Feature extraction\n",
    "\n",
    "With the data loaded, you can now extract features from the texts.\n",
    "As pointed above, for multi-class classification, the features are words frequencies and each topic is represented as a vector of the word frequencies. Let's look once again into *tf-idf* technique.\n",
    "\n",
    "#### Frequency vs count\n",
    "\n",
    "Remember that for text analysis simply counting the occurrences of words is not good enough.\n",
    "An immediate issue is that the longer a text, the higher the word counts in general. Conversely, the shorter the text, the lower the word counts.\n",
    "\n",
    "To solve this issue, you need to divide the word counts by the total number of words in the text.\n",
    "This technique is known as *Term Frequency* or *tf*.\n",
    "\n",
    "#### Common words\n",
    "\n",
    "Another issue arising in text analysis is that even some meaningful words appear in too many texts regardless of a topic – e.g., “issue”, “document”, “human”.\n",
    "\n",
    "For classification, it is important to find distinctive features: words that appear very frequently in some topics and very rarely in others.\n",
    "To downweigh the words that occur in every topic, you need to divide the term frequency in the topic by the frequency across all topics.\n",
    "The technique is called *Term Frequency times Inverse Document Frequency*, or *_tf–idf_*.\n",
    "\n",
    "In `sklearn` you can use it directly.\n",
    "\n",
    "Define a function `text2vec` taking a training set and a test set.\n",
    "\n",
    "* apply a `TfidfVectorizer` with english stop words, fit it on the training set and apply it on both\n",
    "* return the vectorizer as well as the transformed data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here\n",
    "def text2vec(train_set, test_set):\n",
    "    vectorizer = TfidfVectorizer(stop_words = 'english') # Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "    vectors_train = vectorizer.fit_transform(train_set.data)\n",
    "    vectors_test = vectorizer.transform(test_set.data) ################## no FIT in the test set, only in the train set!!!\n",
    "    return (vectorizer, vectors_train, vectors_test)   ################## as we will eveluate our model in TEST based on what\n",
    "                                                       ################## we trained on the TRAIN SET\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First a vectoriser is initialised. Then, it is used to extract features from the dataset.\n",
    "\n",
    "Use this function and print information about the results.\n",
    "In addition, let's estimate the number of non-zero components in the vectors – that is, the words from the joint training and test set vocabulary that occur in the texts of both subsets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data format:\n",
      "(2228, 21639)\n",
      "Non-zero components estimate:\n",
      "52.012567324955114\n",
      "\n",
      "Data format:\n",
      "(1484, 21639)\n",
      "Non-zero components estimate:\n",
      "44.296495956873315\n",
      "------\n",
      "115884\n",
      "2228\n",
      "(2228, 21639)\n",
      "  (0, 9965)\t0.08520864049746116\n",
      "  (0, 10175)\t0.06761187080335423\n",
      "  (0, 2868)\t0.08309262622595794\n",
      "  (0, 18593)\t0.06214733264462163\n",
      "  (0, 8244)\t0.10506063386055675\n",
      "  (0, 12473)\t0.0857849428378622\n",
      "  (0, 7018)\t0.14788831826196372\n",
      "  (0, 9777)\t0.0650365035623811\n",
      "  (0, 20771)\t0.07138269848838813\n",
      "  (0, 3212)\t0.07995776716450928\n",
      "  (0, 17312)\t0.05635066047989126\n",
      "  (0, 6848)\t0.12702864149515555\n",
      "  (0, 19655)\t0.06601701634479967\n",
      "  (0, 21421)\t0.07878341015656297\n",
      "  (0, 15594)\t0.2540572829903111\n",
      "  (0, 5526)\t0.11604463767785615\n",
      "  (0, 19333)\t0.09736700132270687\n",
      "  (0, 15094)\t0.09509934409651985\n",
      "  (0, 3318)\t0.08260500104945903\n",
      "  (0, 16099)\t0.10319417699605977\n",
      "  (0, 13313)\t0.12702864149515555\n",
      "  (0, 16480)\t0.09619264431476056\n",
      "  (0, 19419)\t0.07804622757305782\n",
      "  (0, 12147)\t0.05925816172686216\n",
      "  (0, 3939)\t0.12060341115425738\n",
      "  :\t:\n",
      "  (2227, 18325)\t0.22823829126708084\n",
      "  (2227, 4279)\t0.04049342052312638\n",
      "  (2227, 5560)\t0.22982946765409348\n",
      "  (2227, 13558)\t0.03407512966941616\n",
      "  (2227, 10439)\t0.055603427837397024\n",
      "  (2227, 9703)\t0.05056479066025796\n",
      "  (2227, 9400)\t0.06180102383232532\n",
      "  (2227, 3386)\t0.04187774040330353\n",
      "  (2227, 5652)\t0.06752873705065496\n",
      "  (2227, 11584)\t0.02726579231778806\n",
      "  (2227, 16303)\t0.05494444498633705\n",
      "  (2227, 7503)\t0.051216975515261366\n",
      "  (2227, 20516)\t0.037665839631743185\n",
      "  (2227, 20055)\t0.0400178379366247\n",
      "  (2227, 7199)\t0.026947055082106628\n",
      "  (2227, 11985)\t0.026252793155174845\n",
      "  (2227, 1012)\t0.05077773310508763\n",
      "  (2227, 1182)\t0.04561081492238174\n",
      "  (2227, 11392)\t0.026992003092739317\n",
      "  (2227, 15508)\t0.0682386476655583\n",
      "  (2227, 15203)\t0.04215883152855449\n",
      "  (2227, 7018)\t0.1967348577842451\n",
      "  (2227, 9777)\t0.1730352664700009\n",
      "  (2227, 7327)\t0.11318407328625758\n",
      "  (2227, 7334)\t0.21062608642138114\n"
     ]
    }
   ],
   "source": [
    "categories = ['talk.politics.misc', 'sci.electronics', 'comp.sys.mac.hardware', 'rec.autos']\n",
    "newsgroups_train = load_dataset('train', categories)\n",
    "newsgroups_test = load_dataset('test', categories)\n",
    "\n",
    "vectorizer, vectors_train, vectors_test = text2vec(newsgroups_train, newsgroups_test)\n",
    "\n",
    "print(\"\\nData format:\")\n",
    "print(vectors_train.shape)\n",
    "print(\"Non-zero components estimate:\")\n",
    "print(vectors_train.nnz / float(vectors_train.shape[0]))\n",
    "\n",
    "print(\"\\nData format:\")\n",
    "print(vectors_test.shape)\n",
    "print(\"Non-zero components estimate:\")\n",
    "print(vectors_test.nnz / float(vectors_test.shape[0])) ##### if it is 20 or 30%, the test set is not good as it does not\n",
    "                                                       ##### represent the train set\n",
    "\n",
    "print('------') ################################################# included\n",
    "print(vectors_train.nnz)\n",
    "print(vectors_train.shape[0])\n",
    "print(vectors_train.shape)\n",
    "print(vectors_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pairs in the output indicate the number of texts in each subset and the vocabulary size.\n",
    "Note that test vectors contain more zero elements – they are sparser.\n",
    "\n",
    "\n",
    "### Step 4: Multi-class classification\n",
    "\n",
    "With the feature vectors, you can now implement a classification algorithm.\n",
    "Add the function `classify` to your program:\n",
    "\n",
    "* arguments: `vectors_train`, `vectors_test`, `train_set` and `clf` (an untrained classifier)\n",
    "* fit the classifier on the training data and `train_set.target`\n",
    "* predict and return the prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here\n",
    "def classify(vectors_train, vectors_test, train_set, clf):\n",
    "    clf.fit(vectors_train, train_set.target)\n",
    "    predictions = clf.predict(vectors_test)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes four arguments:\n",
    "\n",
    "- `vectors_train` and `vectors_test` as returned by the vectoriser,\n",
    "- `train_set` for the training set, and\n",
    "- `clf` for the classifier.\n",
    "\n",
    "The classifier is trained against (or fitted to) the training data.\n",
    "Then, the trained classifier is used on the test data.\n",
    "\n",
    "\n",
    "### Step 5: Classification evaluation\n",
    "\n",
    "So far so good: you should be able to run the code above and method `classify` will assign the texts from your test set to one of the four topics each.\n",
    "However, how can you find out if its predictions are any good?\n",
    "Let's call on the `sklearn`'s `metrics.classification_report` to check how well the classifier performs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "comp.sys.mac.hardware       0.86      0.83      0.84       385\n",
      "            rec.autos       0.77      0.87      0.82       396\n",
      "      sci.electronics       0.84      0.76      0.80       393\n",
      "   talk.politics.misc       0.86      0.88      0.87       310\n",
      "\n",
      "             accuracy                           0.83      1484\n",
      "            macro avg       0.84      0.83      0.83      1484\n",
      "         weighted avg       0.83      0.83      0.83      1484\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add your code here\n",
    "predictions = classify(vectors_train, vectors_test, \n",
    "                       newsgroups_train, clf=MultinomialNB(alpha=.01))\n",
    "full_report = classification_report(newsgroups_test.target, \n",
    "                                    predictions, target_names=newsgroups_test.target_names)\n",
    "print(full_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `sklearn` does most of the heavy lifting: all you need to do is call its function with arguments relevant to your problem and pass results around to the next function.\n",
    "This is a blessing because you do not need to worry about the details of the implementation, but also a curse because you are entirely dependent on the library.\n",
    "Thus, it is important to read the [library's documentation](http://scikit-learn.org/stable/modules/classes.html).\n",
    "\n",
    "\n",
    "#### Understanding the evaluation report: accuracy\n",
    "\n",
    "To make more sense of the results reported above, consider first a toy example of classification with two topics: _politics_ (with 90 texts), and _sports_ (with 10).\n",
    "Suppose the classifier report is a table as below:\n",
    "\n",
    "\n",
    "\n",
    "Topics                   | **politics (actual)**|  **sports (actual)**| **Total**\n",
    "--------------------     |:--------------------:|:-------------------:|:---------:\n",
    "**politics (predicted)** |          85          |           6         |   91\n",
    "**sports (predicted)**   |          5           |           4         |   9\n",
    "**Total**                |          90          |           10        |   100\n",
    "\n",
    "The classifier predicts that there are 91 texts on _politics_ and 9 on _sports_.\n",
    "This gives an accuracy of 89%.\n",
    "\n",
    "However, this accuracy is not a good measure of how well – or in this case badly – the classifier performs.\n",
    "Indeed, consider that 90% of texts are about _politics_: a better classifier than the imaginary one above would simply assign all texts to _politics_ and get an accuracy of 90% by doing nothing at all!\n",
    "\n",
    "To better understand the problems of the imaginary classifier above, you must consider additional details: how it performs on each class of text.\n",
    "\n",
    "#### Understanding the evaluation report: true/false positives/negatives\n",
    "\n",
    "The classifier's predictions for any class can be represented as a _confusion matrix_.\n",
    "For example, the confusion matrix for the class $c_1$ is as follows:\n",
    "\n",
    "Classes              |  $c_1$ (actual)  |   $c_2$ (actual)\n",
    "------------------   |:----------------:|:-----------------:\n",
    "$c_1$ (**predicted**)|  true positive   |  false positive\n",
    "$c_2$ (**predicted**)|  false negative  |  true negative\n",
    "\n",
    "* The **true positives** ($\\mathit{tp}$) are the texts that actually belong to $c_1$ and are classified as $c_1$.\n",
    "* The **false positives** ($\\mathit{fp}$) are the texts that actually belong to $c_2$ but are classified as $c_1$.\n",
    "* The **false negatives** ($\\mathit{fn}$) are the texts that actually belong to $c_1$ but are classified as $c_2$.\n",
    "* The **true negatives** ($\\mathit{tn}$) are the texts that actually belong to $c_2$ and are classified as $c_2$.\n",
    "\n",
    "These four numbers exhaustively describe the classifier's performance on texts of $c_1$.\n",
    "\n",
    "#### Understanding the evaluation report: metrics\n",
    "\n",
    "The four numbers $\\mathit{tp}$, $\\mathit{fp}$, $\\mathit{fn}$, and $\\mathit{tn}$ are used to compute more manageable evaluations.\n",
    "Specifically, four distinct metrics are computed from them.\n",
    "\n",
    "* **Precision**: $P = \\frac{\\mathit{tp}}{\\mathit{tp} + \\mathit{fp}}$\n",
    "\n",
    "    The precision is the proportion of true positives among the texts that the classifier predicts belong to $c_1$.\n",
    "    It is a measure of the classifier's _reliability_: how trustworthy the predictions are.\n",
    "\n",
    "* **Recall**: $R = \\frac{\\mathit{tp}}{\\mathit{tp} + \\mathit{fn}}$\n",
    "\n",
    "    The recall is the proportion of true positives among all the texts that actually belong to $c_1$.\n",
    "    It is a measure of your classifier's _coverage_: its ability to find all the relevant cases in the data.\n",
    "\n",
    "* **F1 score**: $\\mathit{F_1} = 2 \\times \\frac{\\mathit{precision} \\times \\mathit{recall}}{\\mathit{precision} + \\mathit{recall}}$\n",
    "\n",
    "    The $F_1$ score is the harmonic mean of the precision and recall metrics.\n",
    "    You can treat it as a measure that reflects your classifier's ability to find all the relevant cases and only those.\n",
    "\n",
    "* **Accuracy**: $\\mathit{Acc} = \\frac{\\mathit{tp} + \\mathit{tn}}{\\mathit{tp} + \\mathit{tn} + \\mathit{fp} + \\mathit{fn}}$\n",
    "\n",
    "    The accuracy is the proportion of correct results.\n",
    "\n",
    "\n",
    "These metrics indicate the different shortcomings of a classifier. E.g., for the imaginary classifier above, the precision, recall and $F_1$ score are:\n",
    "\n",
    "Topics | _politics_  | _sports_  | Average (macro)\n",
    "------ |:-----------:|:---------:|:----------------:\n",
    "P      |  0.930      |   0.440   |   0.685\n",
    "R      |  0.940      |   0.400   |   0.670\n",
    "$F_1$  |  0.935      |   0.420   |   0.678\n",
    "\n",
    "The classifier scores low on all metrics for the sports category.\n",
    "In fact, the precision is below 0.5: the classifier makes more erroneous guesses than correct guesses when presented with sports-related text.\n",
    "\n",
    "\n",
    "#### Reading the evaluation report\n",
    "\n",
    "Running the classifier code with report printing gives the following output:\n",
    "\n",
    "~~~~\n",
    "Full report:\n",
    "                       precision    recall  f1-score   support\n",
    "\n",
    "comp.sys.mac.hardware       0.86      0.83      0.84       385\n",
    "            rec.autos       0.77      0.87      0.82       396\n",
    "      sci.electronics       0.84      0.76      0.80       393\n",
    "   talk.politics.misc       0.86      0.88      0.87       310\n",
    "\n",
    "          avg / total       0.83      0.83      0.83      1484\n",
    "~~~~\n",
    "\n",
    "The `support` column simply reports the number of texts on each topic.\n",
    "As you can see the distribution of texts across four topics is quite balanced, and so is the classifier's performance.\n",
    "\n",
    "\n",
    "**Bonus exercise:**\n",
    "\n",
    ">Use [`sklearn`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)'s functionalities to print the confusion matrix of your classifier for some of the available classes.\n",
    "\n",
    "\n",
    "#### Most predictive features\n",
    "\n",
    "You can access the most predicitve feature of a vectoriser.\n",
    "The function below, `show_top10`, extracts this information. \n",
    "You can also add the call to this function to your `classify` method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_top10(classifier, categories, vectorizer):\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names())\n",
    "####    for i, category in enumerate(categories):\n",
    "    for i, category in enumerate(sorted(categories)): ##################### to FIX the order of the CATEGORIES!!!\n",
    "        top10 = np.argsort(classifier.coef_[i])[-10:]\n",
    "        print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
    "\n",
    "def classify(categories, vectors_train, vectors_test, train_set, clf):\n",
    "    clf.fit(vectors_train, train_set.target)\n",
    "    predictions = clf.predict(vectors_test)\n",
    "    show_top10(clf, categories, vectorizer)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can run the classifier and get the most predictive words per topic along with the full report:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp.sys.mac.hardware: card use monitor know does problem thanks drive apple mac\n",
      "rec.autos: dealer know don new good engine just like cars car\n",
      "sci.electronics: thanks circuit good don used power know does like use\n",
      "talk.politics.misc: know men clinton tax president just think don government people\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "comp.sys.mac.hardware       0.86      0.83      0.84       385\n",
      "            rec.autos       0.77      0.87      0.82       396\n",
      "      sci.electronics       0.84      0.76      0.80       393\n",
      "   talk.politics.misc       0.86      0.88      0.87       310\n",
      "\n",
      "             accuracy                           0.83      1484\n",
      "            macro avg       0.84      0.83      0.83      1484\n",
      "         weighted avg       0.83      0.83      0.83      1484\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add your code here\n",
    "predictions = classify(categories, vectors_train, vectors_test, \n",
    "                       newsgroups_train, clf=MultinomialNB(alpha=.01))\n",
    "full_report = classification_report(newsgroups_test.target, \n",
    "                                            predictions, target_names=newsgroups_test.target_names)\n",
    "print(full_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function stores features in a `numpy` array. It then iterates over the categories, sorting the features by their predictive power each time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language modelling and language generation\n",
    "\n",
    "In all previous modules you worked on language analysis and tried to teach computer how to \"understand\" language.\n",
    "In this module you will look into an opposite task: how to teach computer create or generate language.\n",
    "\n",
    "In essence, as before with the machine learning approaches, computer will learn from experience:\n",
    "having seen many good examples of language use it will try to replicate the most probable phrases.\n",
    "\n",
    "\n",
    "## Text prediction\n",
    "\n",
    "Language generation is widely used in text prediction applications, for example on your mobile phones.\n",
    "\n",
    "![Text prediction](figures/text_prediction.png)\n",
    "\n",
    "\n",
    "**Quiz:**\n",
    "\n",
    ">Can you predict the next character(s) that should follow given string?\n",
    "\n",
    ">* _boo_ ?\n",
    ">* _bootc_ ? ? ?\n",
    "\n",
    ">Can you predict the next word?\n",
    "\n",
    ">* _Happy_ ? ? ? ? ? ? ? ?\n",
    ">* _Good_ ? ? ? ?\n",
    ">* _Have a good_ ? ? ? ? ?\n",
    "\n",
    "**Answer:**\n",
    ">Although you might have come up with different suggestions, with high probability the words that came to your mind are:\n",
    "\n",
    ">* _boo**t**_\n",
    ">* _bootc**amp**_\n",
    ">* _**birthday**_\n",
    ">* _**luck**_\n",
    ">* _**night**_\n",
    "\n",
    "The examples above show that when presented with such riddles we implicitly rely on our observations and knowledge about language.\n",
    "For example, note that:\n",
    "\n",
    "* It is harder to predict the next character or word based on shorter string: for instance, _bo_ can be followed by many more other characters than _boo_.\n",
    "* Some character or word combinations make the prediction almost certain: for instance, once you see _bootc_ or even _bootca_, you are left with a single option of _bootcamp_.\n",
    "* Even when the previous characters or words (called _context_) do not reliably predict the next character or word, they help to eliminate the improbable options:\n",
    "for instance, _a_ is very unlikely to follow _boo_, and _computer_ is quite unlikely to follow _Happy_.\n",
    "* The examples above use unidirectional context and rely on what comes before the character or word.\n",
    "Bidirectional context might help further: it is easier to guess _bootcamp_ if you see _boo_ ?? _amp_ then if you see only _boo_.\n",
    "It is also easier to predict _luck_ if you see \"_Good_ ? ? ? ? *!*\".\n",
    "\n",
    "All these observations, expressed in precise statistical terms, help applications on your computers and mobile phones predict following characters and words.\n",
    "In this module you will use Language Models and implement one in Python to build your own text prediction and text generation application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Models\n",
    "\n",
    "If you want to convert the observations above into more precise statistical form, you will see that one can use _conditional probabilities_ estimated from actual texts to predict the next characters and words.\n",
    "\n",
    "For example, you have a context of \"_Have a good_\" and you would like to predict which word should follow:\n",
    "\n",
    "![Language Model](figures/LM.png)\n",
    "\n",
    "You can look into a big collection of English texts and estimate how probable each of the words is given the context.\n",
    "In other words, you estimate the conditional probabilities $P(day|\\text{Have a good})$, $P(night|\\text{Have a good})$, ..., $P(bootcamp|\\text{Have a good})$.\n",
    "Recall that you have already come across conditional probabilities when you used Naive Bayes classifier.\n",
    "These probabilities can be directly estimated from the data as \n",
    "\n",
    "\\begin{equation}\n",
    "P(day|\\text{Have a good}) = \\frac{num(\\text{Have a good day})}{\\sum_{\\mathit{X} \\in \\mathit{vocabulary}} num(\\text{Have a good X})}\n",
    "\\end{equation}\n",
    "\n",
    "and so on.\n",
    "In other words, the probability of the word \"_day_\" following the context \"_Have a good_\" equals the number of times you see the phrase \"_Have a good day_\"\n",
    "divided by the number of times you see a phrase \"_Have a good_\" followed by all the different words X that can be used in this context.\n",
    "\n",
    "Once the conditional probabilities are estimated, you can pick the most probable next word directly comparing the probability estimates for the word candidates.\n",
    "For example, if you get the following probabilities:\n",
    "\n",
    "![Language Model with probabilities](figures/LM_prob.png)\n",
    "\n",
    "you will choose \"_day_\" as the next word, since $P(day|\\text{Have a good}) > P(night|\\text{Have a good})$ > ... > $P(bootcamp|\\text{Have a good})$.\n",
    "\n",
    "This approach is called *Language Model* and it can be extended to estimate the probabilities of whole sentences.\n",
    "For example, to estimate how probable sentence \"_Have a good day_\" is you use _chain rule_ and multiply the probabilities of the word sequences in the sentence:\n",
    "$P(\\text{Have a good day}) = P(Have) \\times P(a|Have) \\times P(good|\\text{Have a}) \\times P(day|\\text{a good})$.\n",
    "\n",
    "You might have noticed that the formula above uses different number of words in the conditional part:\n",
    "it is one word for $P(a|Have)$ and two for the words that come later.\n",
    "The reason for that is that when we start a sentence, we don't yet have enough context to condition upon.\n",
    "Once we reach the word \"_a_\" we can use one previous word \"_Have_\", and when we move further in the word sequence we can start using longer previous context as condition.\n",
    "\n",
    "Why then don't we use $P(day|\\text{have a good})$ and rather stick to two previous words only?\n",
    "The reason here is that the longer the phrase gets, the less frequent it is in the data: you see \"_a good_\" more often than \"_Have a good_\" simply because the former is a subset of the latter.\n",
    "After certain number of words in the context the probabilities become unreliable, and it is usual to stick to a limited number of words from the previous context to estimate the probabilities.\n",
    "\n",
    "The number of words or symbols used by the Language Model define its order:\n",
    "the model which uses one previous symbol (character or word) is called _unigram_ model,\n",
    "the one that uses two previous symbols – _bigram_, three symbols – _trigram_, and so on.\n",
    "In practice, trigram models are used most frequently.\n",
    "\n",
    "In this module, you will implement your own character-based language model and apply it to generate new text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Language Model in Python\n",
    "\n",
    "Let's start with the import statements:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter #1\n",
    "from numpy import cumsum, sum, searchsorted #2\n",
    "from numpy.random import rand #3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import Python [`defaultdict`](https://docs.python.org/2/library/collections.html) and [`Counter`](https://docs.python.org/2/library/collections.html).\n",
    "You will use them to estimate the probabilities.\n",
    "2. Import numpy's [`cumsum`](https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.cumsum.html),\n",
    "[`sum`](https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.sum.html) and\n",
    "[`searchsorted`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.searchsorted.html).\n",
    "You will use them to retrieve the predictions.\n",
    "3. Import [`rand`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.rand.html)\n",
    "which you will use to generate random numbers.\n",
    "\n",
    "Next, let's define the functionality of the language model.\n",
    "We will define Language Model as a stand-alone Python [class](https://docs.python.org/3/tutorial/classes.html): `class LanguageModel(object)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will allow us to easily call the methods of the `LanguageModel` from other python scripts.\n",
    "To use `LanguageModel` from other scripts you'll need to create `LanguageModel` _objects_ by typing in `objname = LanguageModel(argument)`.\n",
    "_Argument_ is some property of the object that will define your language model.\n",
    "Let's use the order of the model as such important property of the language model object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(object):\n",
    "    def __init__(self, order=1): #1\n",
    "        '''Initializes a language model of the given order.'''\n",
    "        self._transitions = defaultdict(int) #2\n",
    "        self._order = order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The `init` method instantiates an object of the class using the provided arguments.\n",
    "In this case, there's only one argument that you specify – the order of the model, or how far back in the context it will look.\n",
    "By default, you say that the model will rely on _one_ previous symbol (character).\n",
    "`self` here refers to the instance of the class `LanguageModel` object.\n",
    "2. As one of the _fields_ a language model object will keep the transitions from the context to the next character.\n",
    "These transitions will be stored in the Python's `defaultdict`.\n",
    "\n",
    "The objects of class `LanguageModel` should be able to:\n",
    "\n",
    "* _estimate_ the transition probabilities from the context of given order to the next characters based on some training text;\n",
    "* _predict_ the next character based on any new context;\n",
    "* _generate_ a whole sequence using its predictions.\n",
    "\n",
    "To this end let's add three public methods to the class `LanguageModel` that will provide all the functionality described above:\n",
    "\n",
    "* `train` to estimate the probabilities;\n",
    "* `predict` to predict next character;\n",
    "* `generate` to generate a whole sequence of characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(object):\n",
    "    def __init__(self, order=1): #1\n",
    "        '''Initializes a language model of the given order.'''\n",
    "        self._transitions = defaultdict(int) #2\n",
    "        self._order = order\n",
    "        \n",
    "    def train(self, sequence): #1\n",
    "        '''Trains the model using sequence.'''\n",
    "        self._symbols = list(set(sequence))\n",
    "        for i in range(len(sequence)-self._order):\n",
    "            self._transitions[sequence[i:i+self._order], sequence[i+self._order]] += 1 #2\n",
    "\n",
    "    def predict(self, symbol): #3\n",
    "        '''Takes as input a string and predicts the next character.'''\n",
    "        if len(symbol) != self._order:\n",
    "            raise ValueError('Expected string of %d chars, got %d' % (self._order, len(symbol))) #4\n",
    "        probs = [self._transitions[(symbol, s)] for s in self._symbols]\n",
    "        return self._symbols[self._weighted_pick(probs)] #5\n",
    "\n",
    "    def generate(self, start, n): #6\n",
    "        '''Generates n characters from start.'''\n",
    "        result = start\n",
    "        for i in range(n):\n",
    "            new = self.predict(start) #7\n",
    "            result += new\n",
    "            start = start[1:] + new #8\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def _weighted_pick(weights): #9\n",
    "        '''Weighted random selection returns n_picks random indexes.\n",
    "        The chance to pick the index i is given by weights[i].'''\n",
    "        return searchsorted(cumsum(weights), rand()*sum(weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The method `train` learns the transition probabilities from a text, which is represented as a string and provided to the method as an argument `sequence`.\n",
    "2. The `transitions` dictionary keeps the number of times the context of the specified order (length) is followed by the current character.\n",
    "3. The method `predict` chooses the most probable character given the preceding one(s).\n",
    "The preceding one(s) are provided to the method as an argument `symbol`.\n",
    "4. If the length of the provided sequence of the previous characters doesn't match the language model order, report an error.\n",
    "5. Return the character with a given probability.\n",
    "6. The method `generate` allows you to generate a sequence of a specified number (`n`) of characters.\n",
    "7. For that, it calls on the `predict` method providing it with the context `start`.\n",
    "8. It moves the context character by character specified number of `n` times, thus allowing you to generate `n` new characters.\n",
    "9. Method `weighted_pick` provides search functionality for the probabilities.\n",
    "\n",
    "\n",
    "## Using your Language Model in practice\n",
    "\n",
    "Now let’s test how the language model works in practice.\n",
    "You'll start by trying to generate texts using famous books to train the language model.\n",
    "In that case, you should expect to get the generated text that is quite similar in style to the text of the book you trained your language model on.\n",
    "\n",
    "Let's import [urllib](https://docs.python.org/3.0/library/urllib.request.html) that will allow you to access texts from online resources. You can download a book from a collection of [Gutenberg Project books](https://www.gutenberg.org) available online. Let's start with _Robinson Crusoe_:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "655616\n",
      "ï»¿The Project Gutenberg eBook, The Life and Adventures of Robinson Crusoe,\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# add your code to load the page https://www.gutenberg.org/files/521/521-0.txt\n",
    "in_text = requests.get('https://www.gutenberg.org/files/521/521-0.txt').text\n",
    "        \n",
    "print(type(in_text))\n",
    "print(len(in_text))\n",
    "print(in_text[:75])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This piece of code downloads Robinson Crusoe and puts all the contents of this book in a (very long) string.\n",
    "Let's now train your language model and generate new text:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th tthe lllthad hed th whakn he anenâ and te ad d lean th aney t fomecatour,\r\n",
      "s, wrofissuts wa dimy\n"
     ]
    }
   ],
   "source": [
    "model = LanguageModel(order=1) #1\n",
    "model.train(in_text)\n",
    "print (model.generate('t', 100)) #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Here, you use a very simple language model that relies on **one previous character** to generate the **next character**.\n",
    "2. You ask the model to generate a sequence of 100 characters with the starting character \"_t_\".\n",
    "\n",
    "If you run this code, you will get something like that:\n",
    "\n",
    "~~~~\n",
    "t s louma dorg s d axchoy theabexand Brabofoy s they anore s p, gh kind thate, me che weraumen 2.\n",
    "~~~~\n",
    "\n",
    "This doesn’t look readable. Can you see the possible reasons for that?\n",
    "\n",
    "Let's try increasing the order to 2:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if the frovely\r\n",
      "up of mign th, earst my could\r\n",
      "but in the beggertiony carry a Procknound to, had hor? \n"
     ]
    }
   ],
   "source": [
    "model = LanguageModel(order=2) #1\n",
    "model.train(in_text)\n",
    "print (model.generate('if', 100)) #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Order increased to **2 previous characters**.\n",
    "2. Note that the starting context now should also contain at least two characters.\n",
    "\n",
    "Now, if you run this code you might get the following output:\n",
    "\n",
    "~~~~\n",
    "if all thende, I lion, any ths’s\n",
    "as; an une\n",
    "ship, whistrould I seep ints of inved; ot threence be\n",
    "~~~~\n",
    "\n",
    "\n",
    "This is still not quite readable, but you can notice a clear improvement: the words start resembling proper English.\n",
    "Let’s increase the order to 4:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imagined: and bury the dispute, for theirs.  In a word, I told him to know what indefatigable part fire, th\n"
     ]
    }
   ],
   "source": [
    "model = LanguageModel(order=7) #1\n",
    "model.train(in_text)\n",
    "print (model.generate('imagine', 100)) #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time the output looks much better:\n",
    "\n",
    "~~~~\n",
    "your I would gest out of trees, or board of thes,\n",
    "and I might,\n",
    "as for, he wretchest, thought me from s\n",
    "~~~~\n",
    "\n",
    "Nearly all the words generated by the model can be recognised as English words.\n",
    "You can now push your model further and increase the order to 6.\n",
    "Alternatively, you can also change the training data and use some Shakesperean text, for example \"_Romeo and Juliet_\":\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "young Romeo is banishment.\r\n",
      "\r\n",
      "  Prince will to go.\r\n",
      "\r\n",
      "  Lady. Here we have took.\r\n",
      "\r\n",
      "  Friar. O woeful sympathy!\r\n",
      "    A choking gave, that a head at the phrase \"Project Gutenberg Literary Archive Foundation, modified in any man secret and find out logs\r\n",
      "    When I hope thought him. An eagle, madam!\r\n",
      "    Will none, she is come above at night, you in writing with all,\r\n",
      "    And it missheath; but surcease;\r\n",
      "    Ere one can any money, on my children of mine of us, look on here dead, deceas'd; she's debt.                Exit.\r\n",
      "\r\n",
      "  Nurse. Come, death, shall smooth thy will,\r\n",
      "    was this discovered and the tomb;\r\n",
      "    I come. What shall be spent.\r\n",
      "    A sin-absolv'd.\r\n",
      "\r\n",
      "    Where honour man shall have in thine. Things true.\r\n",
      "\r\n",
      "  Ben. Two, two! a shirt and darkness lengthens Romeo.\r\n",
      "\r\n",
      "  Cap. Young men's rattling both!\r\n",
      "    Come you?\r\n",
      "\r\n",
      "  Jul. How now, my liege, my wit! I will still the Project Gutenberg-tm\r\n",
      "electronic works in any work of heaven,\r\n",
      "    Romeo and Men.\r\n",
      "\r\n",
      "  Ben. By my coz.\r\n",
      "\r\n",
      "  Cap. All \n"
     ]
    }
   ],
   "source": [
    "book = 'http://www.gutenberg.org/cache/epub/1112/pg1112.txt' # Romeo and Juliet\n",
    "in_text = requests.get(book).text\n",
    "        \n",
    "model = LanguageModel(order=6)\n",
    "model.train(in_text)\n",
    "print (model.generate('young ', 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might expect to get some interesting output now.\n",
    "\n",
    "As you can see, the model is now able to generate text that resembles the early modern English used by Shakespeare.\n",
    "It has even been able to generate the structure of a dialogue between the characters.\n",
    "\n",
    "The model is still far from generating sensible piece of text – you may notice that it doesn't always respect all the conventions of English grammar and it doesn’t yet write with a purpose. It might also be not as good and convincing as some other [examples of more advanced LMs generating natural text](https://twitter.com/deepdrumpf).\n",
    "Nevertheless, the simplicity of the model with respect to the complexity of its behaviour provides you with a non-trivial fresh perspective on the way language can be generated computationally.\n",
    "\n",
    "\n",
    "## Language Model for review generation\n",
    "\n",
    "Now let's use the `LanguageModel` from this module and `imdb_dataset/pos/` texts as your training data\n",
    "to automatically generate positive movie reviews. You can also experiment with negative movie reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best version of \"A Christmas special relationship between The Empire's all-new half-completely strangle her while Lando Calrissian and women, black and white of heart, it was totally redundant and hope that turns on Luke. There are a good dramas, the audience-pleaser after\" Hollywood\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def read_data(a_dir):\n",
    "    train_data = \"\"\n",
    "    file_list = os.listdir(a_dir)\n",
    "    for a_file in file_list:\n",
    "        f = open(a_dir + a_file, 'r')\n",
    "        train_data += f.read() + \" \"\n",
    "        f.close()\n",
    "    return train_data.strip()\n",
    "\n",
    "# You need to read in all reviews as one string of text\n",
    "# to train the language model on\n",
    "train_data = read_data(\"data/imdb_dataset/pos/\")\n",
    "\n",
    "model = LanguageModel(order=8)\n",
    "model.train(train_data)\n",
    "print (model.generate('the best', 280)) # let's generate a long tweet up to 280 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Language Models\n",
    "\n",
    "For all of the previous models we used some evaluation techniques to assess how well the models perform. The most widely used measurement of the language model performance is called _perplexity_, and it is based on measuring how probable in the language the piece of text generated by a language model is. Let's implement this measure and evaluate the text generated by the language models above.\n",
    "\n",
    "The first step is to collect the data which you will use to calculate probabilities. Use the reviews from either one subset or both subsets of the training data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131105\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def training_data(corpus, a_dir):\n",
    "    file_list = os.listdir(a_dir)\n",
    "    for a_file in file_list[:100]:\n",
    "        f = open(a_dir + a_file, 'r')\n",
    "        corpus += f.read()\n",
    "        f.close()\n",
    "    return corpus\n",
    "\n",
    "training_corpus = training_data(\"\", \"data/imdb_dataset/pos/\")\n",
    "print(len(training_corpus))\n",
    "#training_corpus = training_data(training_corpus, \"imdb_dataset/neg/\")\n",
    "#print(len(training_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the data read in this way contains string of symbols (characters) and before calculating the probabilities of words, you need to tokenise it into constituent words. Let's use spaCy as before:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27018\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "#################nlp = spacy.load('en') #################### it doesnt work!!\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "tokens = nlp(training_corpus)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The language model that you built earlier is very simple – it predicts the next character based on _character n-grams_, i.e. the previous _n_ characters in the sequence. It would be reasonable to evaluate whether it generates a sequence of legitimate English words. For that, you can use _word unigrams_ and estimate whether the language model based on character prediction generates highly probable English words.\n",
    "\n",
    "**Note:**\n",
    ">In practice, _word_ _bi-_ and _tri-grams_ are more frequently used to evaluate text prediction models.\n",
    "\n",
    "Let's collect the statistics on the probability of words from the small training corpus you read in above. In essence, the function below iterates through the word tokens, calculates the number of occurrences for each token and divides it by the total number of occurrences for all word tokens in the corpus assigning probability to the words, i.e. if there are $n$ different word tokens (unigrams) in the corpus, probability of a particular word token (unigram) $P(w)$ is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "P(w) = \\frac{num(w)}{\\sum_{i=1}^{n}num(w_{i})}\n",
    "\\end{equation}\n",
    "\n",
    "In addition, however big the input corpus is, it is hard to expect that you will come across all possible words in this corpus. In practice, it is a good idea to reserve some small amount of probability for the words unseen in the training data, e.g. $\\lambda=0.001$. Remember, you have come across this in the previous module: the technique of reserving some small probability for yet unseen events is called _smoothing_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram(tokens):    \n",
    "    model = defaultdict(lambda: 0.001)\n",
    "    for token in tokens:\n",
    "        token = str(token).strip()\n",
    "        try:\n",
    "            model[token] += 1\n",
    "        except KeyError:\n",
    "            model[token] = 1\n",
    "            continue\n",
    "    total = 0\n",
    "    for word in model:\n",
    "        total+= model.get(word)\n",
    "    for word in model:\n",
    "        model[word] = model.get(word)/float(total)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now calculate _perplexity_. For a string of $N$ words, perplexity is estimated as:\n",
    "\n",
    "\\begin{equation}\n",
    "PP(W) = PP(w_{1}w_{2}...w_{N}) = \\sqrt[N]{\\frac{1}{P(w_{1}w_{2}...w_{N})}} = \\sqrt[N]{\\prod_{i=1}^{N}\\frac{1}{P(w_{i})}}\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(testset, model):\n",
    "    testset = nlp(testset)\n",
    "    perplexity = 1\n",
    "    N = 0\n",
    "    for word in testset:\n",
    "        N += 1\n",
    "        perplexity = perplexity * (1/model[str(word).strip()])\n",
    "    perplexity = pow(perplexity, 1/float(N)) \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that a good language model will generate a highly probable sequence of words. Since the probability is used in denominator, the better the language model, the higher the probability of the sequence – the lower the perplexity value. When you use perplexity to measure the quality of and to compare several language models, you are looking for the one that has **lower perplexity**.\n",
    "\n",
    "With all the components in place, let's apply this measurement and compare a language model that generates text using previous 6 characters with the one that uses previous 8 characters. The more likely the words generated by the language model are, the lower the perplexity score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the beginning , multiple flawless amount would events that he really surnament. If Busy does there, but like a little messanger left me purring Jean Eustache drew member of Peggy grimaces and figures of Chris textures the origines make some occasional quality unraveling often deeper. M\n",
      "800.8826384248341\n",
      "the best gifts he's ever been very busy for Abe - in \"Virginia City\" he grants a pardon to Errol Flynn at the life-style of acting, then see why it's snobbish to convincing Sally Kellerman (who's idea was the baddies and stories Â– a whole series progressing on the story lines and effect\n",
      "439.3490207794224\n"
     ]
    }
   ],
   "source": [
    "lm_data = read_data(\"data/imdb_dataset/pos/\")\n",
    "model = unigram(tokens)\n",
    "\n",
    "lm1 = LanguageModel(order=6)\n",
    "lm1.train(lm_data)\n",
    "testset1 = lm1.generate('the be', 280)\n",
    "print(testset1)\n",
    "print(perplexity(testset1, model))\n",
    "\n",
    "lm2 = LanguageModel(order=8)\n",
    "lm2.train(lm_data)\n",
    "testset2 = lm2.generate('the best', 280)\n",
    "print(testset2)\n",
    "print(perplexity(testset2, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus exercise:**\n",
    ">Implement a _word bi-gram_ based evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The meaning of the word and where to find it\n",
    "\n",
    "The language model above worked reasonably well: note that it is a very simple implementation that relies on just about $100$ lines of Python code, yet it can generate text that, in the first approximation, looks like proper English. Indeed, it follows the rules of English and is mostly grammatically correct. However, presented with such a text one will quickly recognise that the text is written by a machine rather than a human being – that is, the language model that you used will not pass the [Turing test](https://en.wikipedia.org/wiki/Turing_test). \n",
    "\n",
    "**Quiz:**\n",
    ">What is the problem with this approach?\n",
    "\n",
    "The main problem with the language model as implemented above is that it doesn't preserve the _meaning_, especially when it comes to longer pieces of text. Depending on the particular run, your language model might have generated perfectly sensible short expressions like \"_the best part_\", \"_very engaging_\", \"_It's well worth_\" and the like, but combined all together in a sentence or a whole review they rarely contribute to building a coherent story or review. In other words, the model fails to build _global meaning_. \n",
    "\n",
    "**Quiz:**\n",
    ">Can you explain why?\n",
    "\n",
    "In fact, this result is not very surprising: if you look closely into the code, the simple language model above never took the meaning of the words into account in the first place. All it does is trying to \"copy\" what is most probable sequence of characters from the data that you use to train it on. In this module, you will look into how to improve this technology and account for the meaning of words, sentences and even whole texts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantics\n",
    "\n",
    "Whenever we are talking about the meaning of words, phrases, sentences and other \"building blocks\", we are talking about [_semantics_](https://en.wikipedia.org/wiki/Semantics). Semantics is the study of meaning and it deals with how we can represent the meaning of the smaller units such as senquences of characters or words, and how we can derive the meaning of larger units such as phrases and sentences, either on their own or using the meaning of the smaller building blocks. \n",
    "\n",
    "For example, if we agree that the meaning of [_pig_](https://en.wiktionary.org/wiki/pig) is that related to \"any of several intelligent mammalian species of the genus Sus, having cloven hooves, bristles and a nose adapted for digging\", and the meaning of [_small_](https://en.wiktionary.org/wiki/small) is that of \"not large or big; few in numbers or size\", then we can derive the meaning of the phrase _small pig_ by directly combining the meanings of the two building blocks (words) and say that _meaning(\"small pig\") = meaning(\"small\") + meaning(\"pig\")_. This simple technique works quite well on many occasions, and arguably represents how we, humans, derive the meaning of larger expressions: we combine the meanings of the building blocks (words or concepts) where those are compatible. This also suggests that if we know how to represent the meaning of the words computationally, we can always derive the meaning of larger expressions up to sentences and even whole texts computationally or automatically.\n",
    "\n",
    "However, human language and cognition are tricky matters and the simple approach above is not without its weaknesses: for example, the meaning of [_guinea pig_](https://en.wiktionary.org/wiki/guinea_pig) is not a simple combination of the meanings of [_guinea_](https://en.wiktionary.org/wiki/guinea) and _pig_ – in fact, it is an altogether different type of animals unrelated to pigs. This is one of the examples where the simple approach described above will fail. To make things even worse, _guinea pig_ is a building block in itself, and depending on the context of use can mean different things: apart from the animal sense, it can also mean \"a living experimental subject\" well applicable to humans. Words as well as phrases quite often have a variety of meanings – the phenomenon known as [_polysemy_](https://en.wikipedia.org/wiki/Polysemy). Which of the multiple meanings of a word are activated in each particular situation depends on the context of use: we, humans, tend to build mental meaning representations on the go as the story unfolds, however it is much trickier for the machines. It is so challenging that NLP experts even devoted a whole line of research to the task of distinguishing between multiple meanings of words and detecting which one is activated in each particular context of use – the task is called [Word Sense Disambiguation](https://en.wikipedia.org/wiki/Word-sense_disambiguation). At the same time, [_ambiguity_](https://en.wikipedia.org/wiki/Ambiguity) is often used on purpose in creative writing, for rhetorical effect, in jokes and so on: e.g., _“Can you make me a sandwich?” – “Abracadabra! You’re a sandwich!”_.\n",
    "\n",
    "Figurative and non-literal uses of language are especially challegning for the machines to deal with: to wrap up with the pig-related examples, [_pig_](https://en.wiktionary.org/wiki/pig) can be used informally to denote \"a difficult problem\". Finally, sarcasm changes the original meaning of the expressions completely: _\"Well done!\"_ or _\"What a surprise!\"_ might look positive on the paper, but depending on the tone and the situation might mean completely opposite things, and be addressed to a person who did something wrong or used to describe a situation that you expected, predicted or even warned someone about.\n",
    "\n",
    "There are many real-world applications of semantic models. For instance, semantics is used to find relevant information when you submit your queries to the search engine, to recommend you books and movies similar to your preferences, to find duplicates in a set of documents, to automatically generate sensible conversations (think about Siri and Alexa), and so on. The sentiment analyser you implemented earlier didn't use any notion of meaning but it can also benefit from the use of semantic models: for example, if a review contains many occurrences of the word \"_bad_\" it would be classified as a negative review, but what if all those were used in the context of \"_not bad_\"?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words as vectors\n",
    "\n",
    "To teach machines understand human language we need to be able to represent meaning computationally. How should we do that?\n",
    "\n",
    "The field has seen a breakthrough in teaching machines understand human language in 2013 with the publication of two papers by [Tomas Mikolov and his colleagues](https://arxiv.org/pdf/1310.4546.pdf) where they show how to efficiently represent word and phrase meaning computationally. In particular, [one of these papers](https://www.aclweb.org/anthology/N13-1090) presents a now famous example on a word analogy task: if the computer knows that the word _man_ stands in some relation to the word _woman_, given another input word, for example _king_, it can apply the same relation to it and retrieve the word that stands in the same relation to _king_, thus answering the analogy question \"_Man_ is to _woman_ as _king_ is to _ ?\"\n",
    "\n",
    "**Quiz:**\n",
    "> What is the word that should fill in the gap in the question above? What is the relation between the concepts?\n",
    "\n",
    "The most impressive contribution of this work is that the computer **does not know** and **is not being told** what type of relation holds between the two words – it learns it independently by itself using lots of training examples, and having acquired this knowledge can apply this (and many other types of relations) to different pairs of words such as _king_-_queen_, _uncle_-_aunt_, _brother_-_sister_, and so on. The algorithm boils down to:\n",
    "\n",
    "- knowing semantic representations of a wide variety of words, for example `rep(man)`, `rep(woman)`, `rep(king)`, `rep(queen)`, and so on\n",
    "- putting these word representations in a shared semantic space \n",
    "- applying simple algebraic operations to the word representations: e.g., it has been shown that the result of the algebraic operations in the semantic space applied to `rep(king) - rep(man) + rep(woman)` is `rep(queen)`.\n",
    "\n",
    "It can also be argued that our brains implicitly perform similar sort of operations: when looking for the answer for the analogy question above we take the `meaning(king)`, detract the `meaning(man)`, and add `meaning(woman)`, because we know that the word we are looking for shares all the properties of the word _king_ apart from those that distinguish between _man_ and _woman_. Now the central question is what the semantic representations are.\n",
    "\n",
    "**Quiz:**\n",
    "> How do we know who/what the queen is?\n",
    "\n",
    "**Answer:**\n",
    "> We acquire this knowledge over time using rich sources of audio and visual information: we read about kings and queens, we hear aboth them on the TV, the luckiest of us might meet with them personally, and so on.\n",
    "\n",
    "![Who is the queen](figures/queen.png)\n",
    "\n",
    "To teach the computer about the meaning of _king_ and _queen_ we need to provide it with the similar type of information – textual and visual descpription of what it means to be a king or a queen. Since we focus on NLP in this course, we will look into how the computers can learn who a queen is from text.\n",
    "\n",
    "Even though the [Mikolov et al.'s paper](https://www.aclweb.org/anthology/N13-1090) was in many respects groundbreaking, the idea of representing word meaning computationally was not new. One of the most widely cited quotes in semantics is attributed to [John Firth](https://en.wikiquote.org/wiki/John_Rupert_Firth) who postulated back in 1957 (i.e., almost 60 years before that) that \"_You shall know a word by the company it keeps_\". Ever since, it has been the conrnerstone of computational semantics – or the way we represent the meaning of words for computers. In essence, what it says is that to define the meaning of a word or a concept, all we need to do is collect many contexts of use and derive the properties of the word from the neighbouring words.\n",
    "\n",
    "For example, if we provide the computer with a lot of training data – many contexts of use of the word _queen_ – the computer will be able to learn that \"crown\", \"royal\", \"state\", visit\", \"palace\", \"speech\" and other are characteristic properties, attributes and actions of queens. At the same time, the machine will learn that the key difference between kings and queens lies in their gender – the queen will regularly be referred with \"she\" and \"her\", while the king will be referred to as \"he\" or \"his\".\n",
    "\n",
    "![Contexts of use for queen](figures/queen_contexts.png)\n",
    "\n",
    "A very convenient and efficient way to represent word meaning then is _multi-dimensional vector space_ where each dimension represents a different property of the concept (for example, _royalty_ or _femininity_). For decades, computational semantics has been representing semantic space as a multi-dimensional vector space and benefitted from the following properties of such a representation:\n",
    "\n",
    "- **generalisability**: as we pointed out above, it is important to be able to put all words and concepts in the universe in the same space, as it helps to interpret them and establish the relations\n",
    "- **interpretability**: each dimension in the shared semantic space can be interpreted as a particular property so we can easily describe the concepts as collections of certain properties\n",
    "- **visualisation**: vector representations also allow us to interpret the semantic space geometrically. This ranges from visualising the concepts to actually estimating similarity between words using geometrical measures.\n",
    "\n",
    "For example, let's see how the different concepts will be represented in the vector-based semantic space. _Kings_ and _queens_ will share the property of being royal unlike _ministers_:\n",
    "\n",
    "![Royals vs non-royals](figures/royals-vs-nonroyals.png)\n",
    "\n",
    "but at the same time will be differentiated by the gender-related properties:\n",
    "\n",
    "![Males vs females](figures/males-vs-females.png)\n",
    "\n",
    "As we can only visualise up to 3 dimensions in practice, the above are very limited representations of words in the semantic space – try to imagine these in hundreds of thousands of dimensions! These representations, however, visualiase another useful property of the semantic spaces – **similar concepts are located closer together along the relevant dimensions than dissimilar concepts**. For example, _queen_ and _princess_ are located close together because they are both female and share a good deal of semantic meaning, but they will be more distant in space along the age-related dimension. Thus, if two concepts are **very similar** in meaning they will be located close to each other in the space. This gives us the theoretical foundation for how to detect similarity in meaning between words, as well as a practical tool to do that – as we said before, we can interpret the semantic space geometrically and calculate the distances in the semantic space as we would do in the geometric space.\n",
    "\n",
    "Mikolov et al. coined the term _word embeddings_ which are, essentially, word vectors of $300$ dimensions. The developed tool [`word2vec`](link_to_word2vec) is freely available. It arrives at the word embeddings using one of the two regimes: the algorithm builds the target word vector using the context of the surrounding words (`cbow` model predicts the word given its context) or it builds the surrounding word vectors using the input word vector (`skip-gram` model predicts the context given a word):\n",
    "\n",
    "![Skip-gram vs cbow](figures/skipgram-cbow.png)\n",
    "\n",
    "Either way, the resulting vectors are less interpretable: the $300$ dimensions have been empirically found to work well, but each dimension doesn't represent a particular aspect of meaning as with the more traditional and more transparent word vectors. However, the vectors are of high quality – the fact that surrounding contexts are used to learn word vectors guarantees that similar words end up having similar semantic representations, so word embeddings have been demonstrated to perform exceptionally well on many semantic tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to measure similarity\n",
    "\n",
    "The first task that was used to demonstrate usefulness of word embeddings is the word analogy task: _man is to woman what king is to queen_. Once we have the semantic representations for _man_, _woman_ and _king_, the task boils down to two types of operations in the semantic space:\n",
    "\n",
    "- **simple algebraic operations** of detraction [$vector(king)-vector(man)$] and addition [$+ vector(woman)$] on word vectors\n",
    "- **search for the most similar word vector** to the resulting one.\n",
    "\n",
    "The first step is easy – you need to simply detract and sum up the components of the word vectors along each dimension: e.g., if $\\vec{a} = (3, 4)$ and $\\vec{b} = (1, 2)$ then $\\vec{a} - \\vec{b} = (2, 2)$ and $\\vec{a} + \\vec{b} = (4, 6)$. Let's see now how to find the most similar word vector in the semantic space.\n",
    "\n",
    "Since semantic space allows to apply geometric interpretation, we can measure the similarity through the shortest distance in the semantic space and use the approaches that will apply to measuring distances in Eucledean space. In particular, to measure the distance between vectors $\\vec a$ and $\\vec b$ we need to calculate the _cosine_ using the formula:\n",
    "\n",
    "\\begin{equation}\n",
    "cos(\\vec{a}, \\vec{b}) = \\frac{\\vec{a} \\cdot \\vec{b}}{\\lvert \\lvert \\vec{a} \\rvert \\rvert_{2} \\lvert \\lvert \\vec{b} \\rvert \\rvert_{2}} = \\frac{\\sum_{i} (a_{i} \\times b_{i})}{\\sqrt{\\sum_{i}^{n}(a_{i}^{2})} \\times \\sqrt{\\sum_{i}^{n}(b_{i}^{2})}}\n",
    "\\end{equation}\n",
    "\n",
    "<img src=\"figures/cos.png\" width=\"350\">\n",
    "\n",
    "**Quiz:**\n",
    ">What is $cos(\\vec{a}, \\vec{b})$, if $\\vec{a} = (3, 4)$ and $\\vec{b} = (1, 2)$?\n",
    "\n",
    "**Answer:**\n",
    ">\\begin{equation}\n",
    "cos(\\vec{a}, \\vec{b}) =\n",
    "\\frac{\\vec{a} \\cdot \\vec{b}}{\\lvert \\lvert \\vec{a} \\rvert \\rvert_{2} \\lvert \\lvert \\vec{b} \\rvert \\rvert_{2}} =\n",
    "\\frac{\\sum_{i} (a_{i} \\times b_{i})}{\\sqrt{\\sum_{i}^{n}(a_{i}^{2})} \\times \\sqrt{\\sum_{i}^{n}(b_{i}^{2})}} = \n",
    "\\frac{3 \\times 1 + 4 \\times 2}{\\sqrt{3^{2} + 4^{2}} \\times \\sqrt{1^{2} + 2^{2}}} = \n",
    "\\frac{3 + 8}{\\sqrt{25} \\times \\sqrt{5}} = \n",
    "\\frac{11}{\\sqrt{125}} \\approx 0.98\n",
    "\\end{equation}\n",
    "\n",
    "Let's translate these equations into Python code and check whether the answer is correct. For that, you'll use functionality of `numpy` and `math`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9838699100999074\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Implement cosine similarity\n",
    "# add your code here to define a function cosine that takes two vectors and returns the similarity\n",
    "cosine = lambda v1, v2: np.dot(v1, v2)/(np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "#Initialise vectors\n",
    "a = np.fromstring('1 2', dtype=int, sep=' ')\n",
    "b = np.fromstring('3 4', dtype=int, sep=' ')\n",
    "\n",
    "print(cosine(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors with `spaCy`\n",
    "\n",
    "Let's now write some code to access word vectors and measure semantic similarity between words. `spaCy` has nice functionality around that.\n",
    "\n",
    "Let's start by measuring similarity between some sample words. Feel free to experiment with your own ones.\n",
    "\n",
    "**Note**:\n",
    "> You can import the set of models for English that you used for previous modules:\n",
    "\n",
    ">>`import spacy`\n",
    "\n",
    ">>`nlp = spacy.load('en')`\n",
    "\n",
    "> Alternatively, you can use [state-of-the-art models](https://spacy.io/usage/models) trained on Web data for better performance. You'll need to first download them as:\n",
    "\n",
    ">>`python -m spacy download en_core_web_md`\n",
    "\n",
    "> or:\n",
    "\n",
    ">>`pip install spacy`\n",
    "\n",
    ">>`spacy download en_core_web_md`\n",
    "\n",
    "> Once this is done, you can use the models importing the one you'd like: \n",
    "\n",
    ">>`import en_core_web_lg`\n",
    "\n",
    ">>`nlp = en_core_web_lg.load()`\n",
    "\n",
    "> or:\n",
    "\n",
    ">>`import spacy`\n",
    "\n",
    ">>`nlp = spacy.load(''en_core_web_lg')`\n",
    "\n",
    "> for the large-size Web-trained model, `en_core_web_md` for the medium-size one, and `en_core_web_sm` for the small one. The default that is uploaded with `nlp = spacy.load('en')` uses the `en_core_web_sm`.\n",
    "\n",
    ">The difference between the models is largely in performance (expect better results with larger models) and the space requirements: small model takes 35MB while large model takes 812MB. Check [documentation](http://spacy.io/models/en) to make up you mind about the choice of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tbox\tcat\tdog\tapple\torange\tpasta\tpizza\tcoffee\ttea\n",
      "box\t1.0\t0.3235\t0.2777\t0.3119\t0.3377\t0.2454\t0.2852\t0.3636\t0.3115\t\n",
      "cat\t0.3235\t1.0\t0.8017\t0.2821\t0.3288\t0.201\t0.2767\t0.2722\t0.2802\t\n",
      "dog\t0.2777\t0.8017\t1.0\t0.2634\t0.2743\t0.2291\t0.3503\t0.287\t0.294\t\n",
      "apple\t0.3119\t0.2821\t0.2634\t1.0\t0.5619\t0.3593\t0.3902\t0.4266\t0.4289\t\n",
      "orange\t0.3377\t0.3288\t0.2743\t0.5619\t1.0\t0.3344\t0.3134\t0.3907\t0.4201\t\n",
      "pasta\t0.2454\t0.201\t0.2291\t0.3593\t0.3344\t1.0\t0.737\t0.4176\t0.3527\t\n",
      "pizza\t0.2852\t0.2767\t0.3503\t0.3902\t0.3134\t0.737\t1.0\t0.4977\t0.3431\t\n",
      "coffee\t0.3636\t0.2722\t0.287\t0.4266\t0.3907\t0.4176\t0.4977\t1.0\t0.746\t\n",
      "tea\t0.3115\t0.2802\t0.294\t0.4289\t0.4201\t0.3527\t0.3431\t0.746\t1.0\t\n"
     ]
    }
   ],
   "source": [
    "# Import spaCy and initialise the set of models for English\n",
    "import spacy\n",
    "# nlp = spacy.load('en')\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "###############################nlp = spacy.load('en_core_web_sm') #### CHANGED to MD!!\n",
    "\n",
    "text = u'box cat dog apple orange pasta pizza coffee tea' ###################### INCLUDED \"BOX\"!!!!!!!!!!!!!\n",
    "words = nlp(text)\n",
    "\n",
    "print(\"\\t\" + text.replace(\" \", \"\\t\"))\n",
    "\n",
    "# Print out word similarities in a table\n",
    "for word1 in words:\n",
    "    output = str(word1) + \"\\t\"\n",
    "    for word2 in words:\n",
    "        output += str(round(word1.similarity(word2), 4)) + \"\\t\"\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the table is symmetric around the diagonal: that is, `cat.similarity(dog)=dog.similarity(cat)`. The highest value the similarity score can get is $1.0$, and since each word is most similar to itself the diagonal is filled with $1$'s. The next highest similarity score for this set of words is between _cat_ and _dog_. Note that other similarity scores tell us that _pizza_ and _pasta_, and _coffee_ and _tea_ are quite similar to each other – this follows our intuitions, since the first pair describes food and the second describes drinks. \n",
    "\n",
    "You can check your intuitions about word similarity comparing different words. For example, are _cats_ more similar to _dogs_ than they are to _boxes_? (Feel free to check similarity for your own words.)\n",
    "\n",
    "![Cat or dog?](figures/cat-dog.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is statement \"Cat and dog are similar. Cat and frog aren't.\" correct?\n",
      "0.80168545\n",
      "0.43867078\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "statement = u'Cat and dog are similar. Cat and frog aren\\'t.'\n",
    "text = nlp(statement.lower())\n",
    "cat = text[0]\n",
    "dog = text[2]\n",
    "frog = text[8]\n",
    "print (\"Is statement \\\"\" + statement + \"\\\" correct?\")\n",
    "print(cat.similarity(dog))\n",
    "print(cat.similarity(frog))\n",
    "print(cat.similarity(dog) > cat.similarity(frog)) ############## WITH MODEL \"SM\" THE RESULT WAS FALSE!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to 'look under the hood' and see how the word vectors are represented in `spaCy`, you can print them out directly. The code below prints out the first $10$ out of $300$ dimensions: the numbers are not directly interpretable, but note that those for `cat` and `dog` are indeed more similar to each other than to those for `frog`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.15067  -0.024468 -0.23368  -0.23378  -0.18382   0.32711  -0.22084\n",
      " -0.28777   0.12759   1.1656  ]\n",
      "[-0.40176   0.37057   0.021281 -0.34125   0.049538  0.2944   -0.17376\n",
      " -0.27982   0.067622  2.1693  ]\n",
      "[ 0.18531   0.11176  -0.38419  -0.41473  -0.24219   0.37046  -0.42745\n",
      " -0.13813  -0.080212  0.58181 ]\n"
     ]
    }
   ],
   "source": [
    "print (cat.vector[:10])\n",
    "print (dog.vector[:10])\n",
    "print (frog.vector[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word analogy task with `spaCy`\n",
    "\n",
    "Now, let's try to code the word analogy task and see if our algorithm can come up with the solution similar to the one presented in Mikolov et al.'s [paper](https://arxiv.org/pdf/1310.4546.pdf):\n",
    "\n",
    "<img src=\"figures/countries.png\" width=\"600\">\n",
    "\n",
    "That is, our analogy task will encode the relation `country:capital` but the computer won't be explicitly told that this is the relation to be used. Instead we'll ask a question _\"Russia is to Moscow as China is to what?_ (as usual, feel free to insert your own variants). \n",
    "\n",
    "Let's first provide the list of countries and capitals in alphabetical order. To mix things up a bit, let's add some contries (e.g., _Switzerland_ and _Brazil_) with no corresponding capitals on the list, some capitals (e.g., _Amsterdam_ and _London_) with no corresponding countries, and some cities (e.g., _Barcelona_ and _Venice_) that are not capitals. You can always check whether the model has a vector for the word by printing out part of the word vector (always a good idea to check the data you are working with!):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amsterdam\n",
      "[ 0.41373  -0.095219  0.15888  -0.51876   0.41066 ]\n",
      "Ankara\n",
      "[ 0.10742  -0.39872   0.41782   0.80667  -0.022942]\n",
      "Athens\n",
      "[ 0.21507   0.080389  0.25134  -0.3766    0.49439 ]\n",
      "Australia\n",
      "[-0.19401  0.44029  0.18968 -0.52768  0.70956]\n",
      "Barcelona\n",
      "[ 0.36657 -0.29327  0.16112  0.23317  0.54403]\n",
      "Beijing\n",
      "[ 0.78468  0.10513  0.13023 -0.28823  0.19203]\n",
      "Berlin\n",
      "[0.243   0.11444 0.36875 0.2368  0.57666]\n",
      "Brazil\n",
      "[-0.067526  0.050342  0.59258   0.41409   0.6952  ]\n",
      "Chicago\n",
      "[ 0.041337 -0.027126  0.28624  -0.27172   1.0035  ]\n",
      "China\n",
      "[-0.55554   0.32522  -0.090127 -0.49263   0.57665 ]\n",
      "France\n",
      "[-0.16306  0.45292 -0.14638 -0.64332  0.79014]\n",
      "Germany\n",
      "[-0.35532    0.59025    0.17082   -0.0029313  0.73128  ]\n",
      "Greece\n",
      "[ 0.21507   0.080389  0.25134  -0.3766    0.49439 ]\n",
      "Italy\n",
      "[-0.21052    0.18476   -0.0056243 -0.15168    0.78708  ]\n",
      "Japan\n",
      "[-0.44528   -0.17553    0.075346   0.0048481  0.2341   ]\n",
      "Lisbon\n",
      "[ 0.30521 -0.15312 -0.12796  0.42254  0.76491]\n",
      "London\n",
      "[ 0.37837  -0.32807   0.55623  -0.050013  0.72532 ]\n",
      "Madrid\n",
      "[ 0.26001 -0.53344  0.22119 -0.16826  0.58323]\n",
      "Moscow\n",
      "[ 0.11477 -0.22627  0.28292  0.27522  0.28528]\n",
      "Paris\n",
      "[ 0.35213  -0.074228 -0.23725  -0.32726   0.53928 ]\n",
      "Poland\n",
      "[-0.03124  0.10924  0.22819  0.62572  0.93054]\n",
      "Portugal\n",
      "[ 0.30521 -0.15312 -0.12796  0.42254  0.76491]\n",
      "Rome\n",
      "[ 0.41878   0.059621 -0.1914   -0.42063   0.22726 ]\n",
      "Russia\n",
      "[-0.5651   -0.052972  0.16159   0.22136   0.16593 ]\n",
      "Spain\n",
      "[-0.11981   0.011386  0.14965  -0.22285   0.71123 ]\n",
      "Switzerland\n",
      "[ 0.11316  0.16675 -0.28034 -0.18965  0.61872]\n",
      "Tokyo\n",
      "[ 0.28876  -0.55541   0.083178 -0.19359   0.37575 ]\n",
      "Turkey\n",
      "[-0.70281 -0.33607  0.36281  0.52167  0.17502]\n",
      "Venice\n",
      "[ 0.42216    0.0098137  0.50598   -0.05713    0.84544  ]\n",
      "Warsaw\n",
      "[-0.03124  0.10924  0.22819  0.62572  0.93054]\n"
     ]
    }
   ],
   "source": [
    "text = u'Amsterdam Ankara Athens Australia Barcelona Beijing Berlin Brazil Chicago China '\n",
    "text += u'France Germany Greece Italy Japan Lisbon London Madrid Moscow Paris '\n",
    "text += u'Poland Portugal Rome Russia Spain Switzerland Tokyo Turkey Venice Warsaw '\n",
    "words = nlp(text)\n",
    "\n",
    "for word in words:\n",
    "    print(word)\n",
    "    print (word.vector[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are all set to try out the analogy task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Russia is to Moscow as China is to WHAT?\n",
      "Beijing\n"
     ]
    }
   ],
   "source": [
    "question = u\"Russia is to Moscow as China is to WHAT?\"\n",
    "text = nlp(question)\n",
    "source1 = text[0]\n",
    "source2 = text[3]\n",
    "target1 = text[5]\n",
    "\n",
    "max_sim = 0.0\n",
    "target2 = \"N/A\"\n",
    "\n",
    "#Apply the operations on vectors\n",
    "target2_vector = source2.vector-source1.vector+target1.vector\n",
    "\n",
    "#Find the word with the most similar vector to the result\n",
    "for word in words:\n",
    "    if not (str(word)==str(target1) or str(word)==str(source1) or str(word)==str(source2)):\n",
    "        current_sim = cosine(target2_vector, word.vector)\n",
    "        if current_sim >= max_sim:\n",
    "            max_sim = current_sim \n",
    "            target2 = word\n",
    "\n",
    "print(question)\n",
    "print(target2)   ############## WITH MODEL \"SM\" THE RESULT WAS FALSE!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! Now, let's run the task on all countries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Russia is to Moscow as China\n",
      "\t is to Beijing\n",
      "Russia is to Moscow as France\n",
      "\t is to Paris\n",
      "Russia is to Moscow as Germany\n",
      "\t is to Berlin\n",
      "Russia is to Moscow as Greece\n",
      "\t is to Athens\n",
      "Russia is to Moscow as Italy\n",
      "\t is to Rome\n",
      "Russia is to Moscow as Japan\n",
      "\t is to Tokyo\n",
      "Russia is to Moscow as Poland\n",
      "\t is to Warsaw\n",
      "Russia is to Moscow as Portugal\n",
      "\t is to Lisbon\n",
      "Russia is to Moscow as Spain\n",
      "\t is to Barcelona\n",
      "Russia is to Moscow as Turkey\n",
      "\t is to Ankara\n"
     ]
    }
   ],
   "source": [
    "#Define analogy task as a separate method\n",
    "#Note that the code below is almost exactly the same\n",
    "def analogy_task(country):\n",
    "    question = u\"Russia is to Moscow as \" + country\n",
    "    text = nlp(question)\n",
    "    source1 = text[0]\n",
    "    source2 = text[3]\n",
    "    target1 = text[5]\n",
    "\n",
    "    max_sim = 0.0\n",
    "    target2 = \"N/A\"\n",
    "\n",
    "    target2_vector = source2.vector-source1.vector+target1.vector\n",
    "\n",
    "    for word in words:\n",
    "        if not (str(word)==str(target1) or str(word)==str(source1) or str(word)==str(source2)):\n",
    "            current_sim = cosine(target2_vector, word.vector)\n",
    "            if current_sim >= max_sim:\n",
    "                max_sim = current_sim \n",
    "                target2 = word\n",
    "\n",
    "    print(question)\n",
    "    print(\"\\t is to \" + str(target2))\n",
    "    \n",
    "\n",
    "countries = [\"China\", \"France\", \"Germany\", \"Greece\", \"Italy\", \n",
    "             \"Japan\", \"Poland\", \"Portugal\", \"Spain\", \"Turkey\"]\n",
    "\n",
    "for country in countries:\n",
    "    analogy_task(country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the result correspond to the real state-of-affairs? \n",
    "\n",
    "**Bonus:**\n",
    ">Apply the analogy task to pairs of words linked with other types of relations. For inspiration, consider the following examples from Mikolov et al.'s [paper](https://arxiv.org/pdf/1301.3781.pdf):\n",
    "\n",
    "<img src=\"figures/other_relations.png\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document similarity with `spaCy`\n",
    "\n",
    "Now, imagine you have submitted a review to the IMDB and would like to see similar reviews (similarly positive or similarly negative) about a particular movie, or perhaps, you'd simply like to see reviews from like-minded viewers – e.g., similarly positive or negative reviews on similar movies.\n",
    "\n",
    "You can find reviews similar in content and sentiment to yours by applying the `similarity` function to the whole review and finding the other review with the overall highest similarity score to the input one. The code below shows how you can do that. To speed processing up, let's consider only a hundred of the positive and a hundred of the negative reviews. Pick up an input review at random (in the example below, `imdb_dataset/pos/989_9.txt` is used). Note that it is hard to expect that the algorithm will identify a review with a similar sentiment on the *same* movie – after all, the dataset is very sparse with respect to the movies covered! However, let's check whether the algorithm is capable of finding the review with _similar content and sentiment_. In an actual application, this algorithm can be applied to the set of reviews written on the same movie, which will result in finding similar reviews on the same movie across the users.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9695912697908643\n",
      "0.9567454875270055\n",
      "0.9686506430668579\n",
      "0.9797474416415846\n",
      "0.9719893723563529\n",
      "0.9893333062405464\n",
      "0.9856507495199566\n",
      "0.9681738785516374\n",
      "0.9807053018827281\n",
      "0.9557704460124985\n",
      "0.9712462168047581\n",
      "0.9690012352389686\n",
      "0.9534977457772341\n",
      "0.9550070286057575\n",
      "0.9559348805549158\n",
      "0.9771258841199503\n",
      "0.9752825092608302\n",
      "0.9381117380344393\n",
      "0.969137981930322\n",
      "0.9652884340258469\n",
      "0.9708503020533172\n",
      "0.9626264322035399\n",
      "0.959151782298883\n",
      "0.9765290112860704\n",
      "0.9799965214822864\n",
      "0.9812361404315019\n",
      "0.9754487662397097\n",
      "0.9795163788597694\n",
      "0.9693143454070284\n",
      "0.9770310532932374\n",
      "0.9701460688817009\n",
      "0.9685751198800868\n",
      "0.9793509813078454\n",
      "0.9602109265070402\n",
      "0.9525605770440329\n",
      "0.9806463081701483\n",
      "0.9641500958687438\n",
      "0.966294542387178\n",
      "0.9698115425502075\n",
      "0.9803168731197518\n",
      "0.9493309098773883\n",
      "0.984583202864715\n",
      "0.9876059386484985\n",
      "0.979999486958209\n",
      "0.9770760019827816\n",
      "0.9681457765500985\n",
      "0.9364873956393414\n",
      "0.955160883350104\n",
      "0.9861277979992421\n",
      "0.9839599348291671\n",
      "0.9648777880203256\n",
      "0.9626228704825024\n",
      "0.9762049909630514\n",
      "0.956421695682703\n",
      "0.9736096682099057\n",
      "0.9767749925588238\n",
      "0.9539483062462314\n",
      "0.9671013161801039\n",
      "0.9828805345868674\n",
      "0.9688692211781026\n",
      "0.9702798269341064\n",
      "0.9528337760452353\n",
      "0.9783777289403147\n",
      "0.9681174638354962\n",
      "0.9737481345628287\n",
      "0.9661242343532445\n",
      "0.9772434067418571\n",
      "0.9667189086903972\n",
      "0.9588826109131823\n",
      "0.972375393575027\n",
      "0.9693065277311458\n",
      "0.9706703379645458\n",
      "0.9306926972882104\n",
      "0.9736187427401508\n",
      "0.9539232772213154\n",
      "0.9718932946644905\n",
      "0.9708949868214118\n",
      "0.9800621147686838\n",
      "0.9755995488237449\n",
      "0.973610520563764\n",
      "0.9787430446944375\n",
      "0.9802071812094788\n",
      "0.9679323766128515\n",
      "0.9710692080262692\n",
      "0.976440951443068\n",
      "0.971635425987122\n",
      "0.9643660521697303\n",
      "0.9766390130741235\n",
      "0.9625727188246257\n",
      "0.9846465396856813\n",
      "0.9669475482689754\n",
      "0.9633346784546823\n",
      "0.9334248963971422\n",
      "0.9537800876784088\n",
      "0.9784120372905801\n",
      "0.9786442947690893\n",
      "0.9559493758838561\n",
      "0.9834139762239569\n",
      "0.9747998356262135\n",
      "0.9666320771177827\n",
      "========\n",
      "NEGATIVE\n",
      "========\n",
      "0.9757206777313477\n",
      "0.9779622297322739\n",
      "0.9798267597279745\n",
      "0.9710567932184261\n",
      "0.9755842024635681\n",
      "0.9736400515108814\n",
      "0.9770272851696732\n",
      "0.965410113408016\n",
      "0.976513430040468\n",
      "0.9748274944331602\n",
      "0.930975121486514\n",
      "0.9856834422426266\n",
      "0.9780349016916026\n",
      "0.9750818032743304\n",
      "0.9786557203372512\n",
      "0.9701369918747232\n",
      "0.9746991918245299\n",
      "0.962403541145176\n",
      "0.9843645117306938\n",
      "0.9865451778490336\n",
      "0.9782566574113811\n",
      "0.9793059806891143\n",
      "0.9759536298166012\n",
      "0.975754815476902\n",
      "0.9780821747200344\n",
      "0.9750032805201728\n",
      "0.9732157339396764\n",
      "0.975240741679398\n",
      "0.9705784675329567\n",
      "0.9652047090065445\n",
      "0.9776463039871679\n",
      "0.9567176533959915\n",
      "0.9813668525905167\n",
      "0.9823794126493236\n",
      "0.9818011576050935\n",
      "0.9738847921728887\n",
      "0.965892168995758\n",
      "0.982034219492896\n",
      "0.9792730960672604\n",
      "0.9822843309488551\n",
      "0.9786234686193002\n",
      "0.9726726907244609\n",
      "0.9696933501240199\n",
      "0.9804678093262101\n",
      "0.9735484517033977\n",
      "0.967594888550756\n",
      "0.9556444103967796\n",
      "0.9821126407894962\n",
      "0.9695293786222785\n",
      "0.9798714535549679\n",
      "0.9767683320936604\n",
      "0.9660928336398688\n",
      "0.9739372654515281\n",
      "0.9836675711762685\n",
      "0.9792397246095387\n",
      "0.973216046407489\n",
      "0.9650257107903211\n",
      "0.9815254329053753\n",
      "0.9811931549467596\n",
      "0.9659961944254258\n",
      "0.9820693847005918\n",
      "0.9749551740600786\n",
      "0.9778040054149715\n",
      "0.9823818835960588\n",
      "0.9647787328134628\n",
      "0.9680856371289274\n",
      "0.9824997696706472\n",
      "0.972543580204643\n",
      "0.973477402256072\n",
      "0.9744609042874818\n",
      "0.962986078680561\n",
      "0.974472938795793\n",
      "0.9811955258182068\n",
      "0.9271978027841197\n",
      "0.9793099148549731\n",
      "0.9723351392641733\n",
      "0.9767602663477538\n",
      "0.9827703741067748\n",
      "0.9775033085261626\n",
      "0.9612904702943944\n",
      "0.9843829999615898\n",
      "0.9770715722402118\n",
      "0.987881500810634\n",
      "0.9784418206971104\n",
      "0.9808799049768943\n",
      "0.9801974214023152\n",
      "0.9760028868117115\n",
      "0.9772643759964861\n",
      "0.945727293509394\n",
      "0.9759408475184693\n",
      "0.9768731938770099\n",
      "0.9777321528766209\n",
      "0.9729503744280574\n",
      "0.9504469041809006\n",
      "0.9760244630385809\n",
      "0.9695223811288463\n",
      "0.9585409802642344\n",
      "0.9758547044994899\n",
      "0.975223049145398\n",
      "0.9721918196557798\n",
      "The most similar review to:\n",
      "\n",
      "Both my friend and I thought this movie was well done. We expected a light hearted comedy but got a full blown action movie with comic thrusts. We both thought that this movie may have not done so well at the box office as the previews lead us to believe it was a comedy. I was impressed with the supporting actors and of course Dave Morse always puts in a terrific acting job. Most of the supporting cast are veterans not first timers and they were solid. We both felt that the writing and direction were first rate and made comments to each other about buying this movie. If you don't buy rent it for a good time.\n",
      "\n",
      "is:\n",
      "\n",
      "This movie had an interesting cast, it mat not have had an a list cast but the actors that were in this film did a good job. Im glad we have b grade movies like this one, the story is basic the actors are basic and so is the way they execute it, you don't need a million dollar budget to make a film just a mix of b list ordinary actors and a basic plot. I like the way they had the street to themselves and that there was no one else around and also what i though was interesting is that they didn't close down a cafÃ© to set there gear and that they did it all from a police station. Arnold vosloo and Michael madsen did a great job at portraying there roles in the hostage situation. This was a great film and i hope to see more like it in the near future.\n",
      "pos/104_10.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import spacy\n",
    "################################nlp = spacy.load('en')\n",
    "nlp = spacy.load('en_core_web_md') ##################################### CHANGED!!\n",
    "\n",
    "\n",
    "def init_dict(a_dir):\n",
    "    a_dict = {}\n",
    "    file_list = os.listdir(a_dir)\n",
    "    for a_file in file_list[:100]:\n",
    "        if not a_file.startswith(\".\"):\n",
    "            f = open(a_dir + a_file, 'r')\n",
    "            a_dict[a_file] = nlp(f.read())\n",
    "            f.close()\n",
    "    return a_dict\n",
    "\n",
    "\n",
    "pos = init_dict(\"data/imdb_dataset/pos/\")\n",
    "neg = init_dict(\"data/imdb_dataset/neg/\")\n",
    "\n",
    "f = open(\"data/imdb_dataset/pos/989_9.txt\", 'r')\n",
    "review = nlp(f.read())\n",
    "f.close()\n",
    "\n",
    "max_sim = 0.0\n",
    "similar = \"N/A\"\n",
    "similar_path = \"N/A\"\n",
    "for rev in pos.keys():\n",
    "    rev_text = pos.get(rev)\n",
    "    current_sim = review.similarity(rev_text)\n",
    "\n",
    "    print(current_sim)\n",
    "    \n",
    "    if current_sim >= max_sim:\n",
    "        max_sim = current_sim\n",
    "        similar = rev_text\n",
    "        similar_path = \"pos/\" + rev\n",
    "\n",
    "print('========')\n",
    "print('NEGATIVE')\n",
    "print('========')\n",
    "\n",
    "for rev in neg.keys():\n",
    "    rev_text = neg.get(rev)\n",
    "    current_sim = review.similarity(rev_text)\n",
    "\n",
    "    print(current_sim)    \n",
    "    \n",
    "    if current_sim >= max_sim:\n",
    "        max_sim = current_sim\n",
    "        similar = rev_text\n",
    "        similar_path = \"neg/\" + rev\n",
    "\n",
    "        \n",
    "print(\"The most similar review to:\\n\")\n",
    "print(review[:1000])\n",
    "print(\"\\nis:\\n\")\n",
    "print(similar[:1000])\n",
    "print(similar_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "This is an art film that was either made in 1969 or 1972 (the National Film Preservation Foundation says 1969 and IMDb says 1972). Regardless of the exact date, the film definitely appears to be very indicative of this general time period--with some camera-work and pop art stylings that are pure late 60s-early 70s.  The film consists of three simple images that are distorted using different weird camera tricks. These distorted images are accompanied by music and there is absolutely no dialog or plot of any sort. This was obviously intended as almost like a form of performance art, and like most performance art, it's interesting at first but quickly becomes tiresome. The film, to put it even more bluntly, is a total bore and would appeal to no one but perhaps those who made the film, their family and friends and perhaps a few people just too hip and \"with it\" to be understood by us mortals."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9721918196557798"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.similarity(rev_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 5\n"
     ]
    }
   ],
   "source": [
    "#### https://dzone.com/articles/how-related-your-documents-are-measure-document-si\n",
    "import glob\n",
    "import os\n",
    "file_list = glob.glob(os.path.join(os.getcwd(), \"doccomparison/\", \"*.txt\"))\n",
    "raw_documents = []\n",
    "for file_path in file_list:\n",
    "    with open(file_path, encoding=\"utf8\") as f_input:\n",
    "        raw_documents.append(f_input.read())\n",
    "print(\"Number of documents:\",len(raw_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['data', 'science', 'sits', 'at', 'the', 'core', 'of', 'any', 'analytical', 'exercise', 'conducted', 'on', 'a', 'big', 'data', 'or', 'internet', 'of', 'things', '(', 'iot', ')', 'environment', '.', 'data', 'science', 'involves', 'a', 'wide', 'array', 'of', 'technologies', ',', 'business', ',', 'and', 'machine', 'learning', 'algorithms', '.', 'the', 'purpose', 'of', 'data', 'science', 'is', 'just', 'not', 'doing', 'machine', 'learning', 'or', 'statistical', 'analysis', 'but', 'also', 'to', 'derive', 'insights', 'out', 'of', 'the', 'data', 'that', 'a', 'user', 'with', 'no', 'statistics', 'knowledge', 'can', 'understand', '.', 'in', 'a', 'fast', 'paced', 'environment', 'such', 'as', 'big', 'data', 'and', 'iot', 'where', 'the', 'type', 'of', 'data', 'might', 'vary', 'over', 'the', 'course', 'of', 'time', ',', 'it', 'becomes', 'difficult', 'to', 'maintain', 'and', 'recreate', 'the', 'models', 'each', 'and', 'every', 'time.this', 'gap', 'calls', 'up', 'for', 'an', 'automated', 'way', 'to', 'manage', 'the', 'data', 'science', 'algorithms', 'in', 'those', 'environments', '.', 'the', 'rise', 'of', 'data', 'science', 'was', 'to', 'move', 'away', 'from', 'a', 'rule-based', 'system', 'to', 'a', 'system', 'where', 'a', 'machine', 'learn', 'rules', 'by', 'itself', 'for', 'its', 'automation', '.', 'machine', 'learning', 'makes', 'data', 'science', 'inherently', 'partially', 'automated', '.', 'the', 'other', 'half', 'which', 'needs', 'manual', 'intervention', 'is', 'still', 'to', 'be', 'automated', '.', 'however', ',', 'those', 'are', 'the', 'areas', 'which', 'involve', 'the', 'experience', 'and', 'wisdom', 'of', 'a', 'data', 'scientist', ',', 'a', 'business', 'expert', ',', 'a', 'software', 'developer', ',', 'a', 'data', 'integrator', ',', 'each', 'and', 'everyone', 'who', 'currently', 'contribute', 'to', 'making', 'a', 'data', 'science', 'project', 'operational', '.', 'this', 'makes', 'it', 'difficult', 'to', 'automate', 'each', 'and', 'every', 'aspect', 'of', 'it', '.', 'however', ',', 'we', 'can', 'think', 'of', 'data', 'science', 'automation', 'as', 'a', 'two-level', 'architecture', 'wherein', ':', 'level-1', ':', 'different', 'data', 'science', 'disciplines/components', 'are', 'automated', '.', 'level-2', ':', 'all', 'the', 'individual', 'automated', 'components', 'are', 'interconnected', 'to', 'form', 'a', 'coherent', 'data', 'science', 'system', '.', 'we', 'can', 'think', 'of', 'an', 'automated', 'data', 'science', 'system', 'capable', 'enough', 'to', 'articulate', 'us', 'our', 'problem', 'whenever', 'we', 'throw', 'a', 'data', 'set', 'to', 'it', '.', 'also', ',', 'it', 'should', 'be', 'intelligent', 'enough', 'to', 'provide', 'us', 'with', 'all', 'possible', 'solutions', 'in', 'a', 'language', 'which', 'we', 'can', 'understand', '.', 'we', 'can', 'imagine', 'data', 'preparation', ',', 'machine', 'learning', ',', 'domain', 'knowledge', ',', 'and', 'result', 'interpretation', 'as', 'four', 'major', 'tasks', 'required', 'to', 'execute', 'a', 'data', 'science', 'project', 'successfully', '.', 'all', 'these', 'tasks', 'have', 'to', 'be', 'converted', 'to', 'automated', 'modules', 'to', 'create', 'an', 'automated', 'data', 'science', 'system', '.', 'data', 'preparation', 'automation', ':', 'data', 'preparation', 'is', 'a', 'repetitive', 'task', 'that', 'has', 'to', 'be', 'done', 'every', 'time', 'while', 'creating', 'models', '.', 'data', 'extraction', ',', 'data', 'cleaning', ',', 'and', 'data', 'transformations', 'such', 'as', 'imputing', 'null', 'values', 'and', 'algorithm', 'specific', 'transformations', 'are', 'some', 'tasks', 'which', 'fall', 'into', 'this', 'category', '.', 'many', 'organizations', 'have', 'adopted', 'to', 'automate', 'these', 'tasks', 'and', 'branded', 'the', 'engine', 'as', 'a', 'data', 'science', 'automation', 'tool', '.', 'however', ',', 'most', 'of', 'these', 'tools', 'use', 'rule-based', 'logic', 'for', 'automating', 'the', 'data', 'preprocessing', 'tasks', '.', 'a', 'question', ':', 'do', 'we', 'need', 'rule-based', 'systems', 'to', 'automate', 'data', 'science', '?', 'well', ',', 'no', '.', 'we', 'need', 'data', 'preprocessing', 'automated', 'by', 'machine', 'learning', 'itself', '.', 'for', 'example', ',', 'the', 'decision', 'to', 'choose', 'what', 'all', 'preprocessing', 'function', 'has', 'to', 'be', 'applied', 'on', 'the', 'data', 'for', 'a', 'problem', 'are', 'to', 'be', 'learned', 'by', 'machines', 'themselves', '.', 'feature', 'engineering', 'is', 'an', 'another', 'area', 'of', 'data', 'preparation', 'which', 'requires', 'automation', '.', 'feature', 'engineering', 'is', 'a', 'technique', 'to', 'convert', 'raw', 'data', 'into', 'attributes/predictors', 'that', 'contribute', 'to', 'improving', 'the', 'accuracy', 'of', 'a', 'machine', 'learning', 'project', '.', 'feature', 'engineering', 'automation', 'is', 'still', 'at', 'a', 'nascent', 'stage', 'and', 'an', 'active', 'area', 'of', 'research', '.', 'incredible', 'progress', 'is', 'made', 'by', 'data', 'scientists', 'from', 'mit', 'on', 'developing', 'a', '``', 'deep', 'feature', 'synthesis', \"''\", 'algorithm', 'capable', 'of', 'generating', 'features', 'from', 'raw', 'data', '.', 'automated', 'machine', 'learning/statistician', ':', 'this', 'is', 'an', 'area', 'of', 'data', 'science', 'automation', 'where', 'statistical', 'routines', 'and', 'machine', 'learning', 'are', 'automated', '.', 'the', 'system', 'executes', 'the', 'best', 'algorithm', 'based', 'on', 'the', 'provided', 'data', 'set', '.', 'it', 'hides', 'the', 'intricacies', 'and', 'mathematical', 'complexity', 'of', 'algorithms', 'from', 'the', 'user', 'making', 'it', 'available', 'to', 'masses', '.', 'the', 'user', 'needs', 'to', 'provide', 'automated', 'statistician', 'with', 'data', '.', 'it', 'understands', 'the', 'data', ',', 'creates', 'different', 'mathematical', 'models', 'and', 'returns', 'the', 'result', 'based', 'on', 'a', 'model', 'that', 'best', 'explains', 'the', 'data', '.', 'automated', 'statistician', 'is', 'a', 'complex', 'science', 'as', 'it', 'requires', 'the', 'system', 'to', 'learn', 'the', 'input', 'data', 'patterns', ',', 'find', 'the', 'best', 'fit', 'values', 'and', 'self-optimize', 'its', 'parameters', 'using', 'several', 'statistical', 'and', 'machine', 'learning', 'algorithms', '.', 'this', 'requires', 'the', 'generalization', 'of', 'various', 'algorithm', 'constraints', 'and', 'enormous', 'computing', 'power', '.', 'automated', 'machine', 'learning', 'is', 'gradually', 'maturing', 'by', 'leveraging', 'cloud-based', 'servers', 'to', 'manage', 'the', 'requirement', 'for', 'high', 'computational', 'power', '.', 'organizations', 'creating', 'data', 'products', 'are', 'progressively', 'including', 'features', 'such', 'as', 'meta-learning', ',', 'a', 'process', 'of', 'automatically', 'selecting', 'a', 'suitable', 'machine', 'learning', 'algorithm', 'based', 'on', 'the', 'metadata', 'of', 'the', 'data', 'set', '.', 'also', ',', 'there', 'has', 'been', 'a', 'breakthrough', 'of', 'bringing', 'neural', 'networks', 'and', 'deep', 'learning', 'to', 'mainstream', 'for', 'automated', 'data', 'science', 'tasks', '.', 'some', 'of', 'the', 'niche', 'artifical', 'intelligence', 'startups', 'like', 'h2o.ai', 'are', 'the', 'forerunners', 'of', 'creating', 'in-memory', 'optimized', 'deep', 'learning', 'and', 'machine', 'learning', 'algorithms', ',', 'generalized', 'model', 'building', 'process', 'by', 'introducing', 'several', 'built-in', 'functionalities', 'and', 'providing', 'many', 'model', 'tuning', 'options', 'such', 'as', 'hyperparameter', 'tuning', 'which', 'support', 'to', 'have', 'greater', 'control', 'over', 'the', 'algorithms', '.', 'hyperparameter', 'tuning', 'is', 'a', 'process', 'to', 'automate', 'the', 'trial', 'and', 'error', 'of', 're-running', 'the', 'machine', 'learning', 'models', 'several', 'times', 'to', 'decide', 'the', 'appropriate', 'parameters', 'for', 'a', 'model', 'on', 'a', 'data', 'set.this', 'helps', 'in', 'automating', 'data', 'science', 'process', 'to', 'a', 'certain', 'extent', 'and', 'make', 'data', 'scientists', 'free', 'from', 'cumbersome', 'process', 'of', 'testing', 'the', 'models', 'with', 'different', 'parameters', '.', 'insights', 'generation', 'automation', ':', 'results', 'out', 'of', 'a', 'data', 'science', 'project', 'are', 'not', 'useful', 'until', 'and', 'unless', 'a', 'business', 'user', 'or', 'an', 'audience', 'with', 'no', 'statistics', 'knowledge', 'comprehends', 'it', '.', 'the', 'cream', 'of', 'a', 'data', 'science', 'activity', 'is', 'the', 'story', 'telling', 'part', 'where', 'a', 'data', 'scientist', 'explains', 'the', 'results', 'to', 'people', 'in', 'a', 'comprehensive', 'and', 'transparent', 'method', '.', 'automating', 'this', 'task', 'requires', 'generating', 'user-friendly', 'texts', 'automatically', 'from', 'the', 'statistician', 'friendly', 'results', '.', 'natural', 'language', 'generation', '(', 'nlg', ')', 'is', 'the', 'current', 'front', 'runner', 'framework', 'which', 'can', 'help', 'in', 'translating', 'machine', 'language', 'into', 'a', 'natural', 'language', '.', 'nlgserv', 'and', 'simplenlg', 'are', 'two', 'nlg', 'frameworks', 'which', 'we', 'can', 'use', 'for', 'this', 'task', '.', 'also', ',', 'we', 'can', 'use', 'markov', 'chains', 'for', 'generating', 'sentences', 'and', 'for', 'manufacturing', 'stories', 'automatically', '.', 'to', 'conclude', ',', 'we', 'can', 'say', 'the', 'innovation', 'for', 'data', 'science', 'automation', 'has', 'started', 'coming', 'into', 'existence', 'and', 'will', 'gradually', 'evolve', 'in', 'years', 'to', 'come', '.', 'we', 'are', 'currently', 'at', 'a', 'stage', 'where', 'we', 'have', 'begun', 'to', 'tackle', 'automation', 'for', 'individual', 'data', 'science', 'modules', '.', 'from', 'here', ',', 'we', 'need', 'to', 'move', 'to', 'a', 'more', 'generic', 'data', 'science', 'platform', 'with', 'all', 'modules', 'automated', 'and', 'integrated', 'together', '.', 'this', 'is', 'how', 'a', 'change', 'starts', '.', 'just', 'like', 'the', 'way', ',', 'the', 'room-sized', 'computers', 'with', 'ancillary', 'parts', 'got', 'transformed', 'to', 'tiny', 'credit-card', 'sized', 'computers', 'like', 'raspberry', 'pi', '.'], ['year', 'after', 'year', ',', 'data', 'science', 'techniques', 'mature', 'and', 'deliver', 'outstanding', 'results', 'with', 'successful', 'implementations', '.', '2016', 'ends', 'with', 'organizations', 'embracing', 'big', 'data', 'analytics', ',', 'artificial', 'intelligence', ',', 'and', 'data', 'science', 'as', 'key', 'differentiators', 'in', 'their', 'business', 'processes', '.', 'there', 'have', 'been', 'various', 'developments', 'in', 'the', 'field', 'of', 'data', 'science', 'and', 'related', 'technologies', '.', 'we', 'saw', 'growth', 'in', 'data', 'scientists', 'from', 'all', 'fields', 'of', 'profession', 'and', 'study', '.', 'most', 'consulting', 'enterprises', 'established', 'analytics', 'and', 'data', 'science', 'as', 'one', 'of', 'their', 'key', 'offerings', ',', 'with', 'many', 'niche', 'startups', 'mushrooming', 'to', 'grab', 'a', 'space', 'in', 'this', 'area', '.', 'the', 'advantage', 'that', 'we', 'have', 'is', 'the', 'increased', 'contribution', 'of', 'the', 'data', 'scientists', 'to', 'open', 'source', 'development', 'communities', ',', 'laying', 'out', 'new', 'thought', 'processes', 'in', 'the', 'analytics', 'industry', 'and', 'bringing', 'out', 'innovative', 'ways', 'to', 'solve', 'business', 'problems', '.', 'these', 'factors', 'with', 'other', 'emerging', 'technologies', 'like', 'cloud', ',', 'big', 'data', ',', 'and', 'mobile', 'technologies', 'have', 'led', 'to', 'the', 'evolution', 'of', 'new', 'trends', 'in', 'data', 'science', '.', 'let', 'us', 'look', 'at', 'a', 'high', 'level', 'evolution', 'of', 'data', 'science', 'and', 'its', 'related', 'technologies', 'in', 'the', 'past', ',', 'and', 'discuss', 'new', 'emerging', 'trends', 'in', 'the', 'field', 'of', 'data', 'science', ':', 'data', 'science', ':', 'first', 'appeared', 'as', 'a', 'term', 'in', 'early', '1997', 'in', 'a', 'lecture', 'by', 'c.f', '.', 'jeff', 'wu', ',', 'called', '“', 'statistics', '=', 'data', 'science', '?', '”', '.', 'in', '2008', ',', 'dj', 'patil', 'and', 'jeff', 'hammerbacher', 'for', 'the', 'first', 'time', 'used', 'the', 'words', '“', 'data', 'scientists', '”', 'to', 'describe', 'their', 'teams', '.', 'the', 'year', '2010', 'marked', 'the', 'rise', 'of', 'data', 'science', 'and', 'data', 'scientists', 'when', 'enterprises', 'started', 'to', 'practice', 'it', 'in', 'their', 'trades', '.', 'big', 'data', 'analytics', ':', 'the', 'rise', 'of', 'data', 'science', 'is', 'coupled', 'with', 'the', 'growth', 'of', 'big', 'data', '.', 'big', 'data', 'is', 'a', 'termed', 'coined', 'for', 'an', 'amount', 'of', 'data', 'with', 'high', 'volume', ',', 'velocity', ',', 'and', 'veracity', '.', 'though', 'in', 'the', 'past', ',', 'we', 'had', 'sufficient', 'space', 'available', 'to', 'store', 'big', 'data', ',', 'but', 'there', 'was', 'limited', 'access', 'to', 'analyze', 'and', 'process', 'this', 'data', '.', 'for', 'the', 'first', 'time', 'in', '2004', ',', 'google', 'published', 'research', 'papers', 'on', 'google', 'file', 'system', '(', 'gfs', ')', 'to', 'store', 'and', 'process', 'big', 'data', '.', 'later', ',', 'in', '2006', ',', 'yahoo', 'developed', 'its', 'first', 'prototype', 'on', 'hadoop', ',', 'which', 'was', 'open', 'sourced', 'and', 'became', 'the', 'heart', 'of', 'big', 'data', 'analytics', '.', 'for', 'a', 'data', 'scientist', ',', 'it', 'is', 'of', 'the', 'utmost', 'importance', 'to', 'learn', 'techniques', 'to', 'tap', 'into', 'the', 'intelligence', 'from', 'big', 'data', '.', 'this', 'requires', 'them', 'to', 'understand', 'and', 'execute', 'the', 'machine', 'learning', 'algorithms', 'and', 'data', 'science', 'techniques', 'in', 'these', 'big', 'data', 'analytics', 'platforms', 'and', 'tools', '.', 'cloud', 'and', 'data', 'science', ':', 'cloud', 'reduced', 'the', 'overall', 'cost', 'of', 'infrastructure', ',', 'software', ',', 'and', 'platforms', '.', 'this', 'makes', 'it', 'ideal', 'for', 'analyzing', 'big', 'data', 'and', 'storing', 'large', 'sets', 'of', 'data', 'online', 'at', 'a', 'reduced', 'cost', 'and', 'minimum', 'maintenance', 'overhead', '.', 'the', 'cloud-based', 'data', 'science', 'and', 'machine', 'learning', 'platforms', 'provide', 'an', 'ideal', 'environment', 'for', 'data', 'scientists', 'to', 'access', 'data', 'stored', 'in', 'the', 'cloud', ',', 'process', 'it', ',', 'and', 'analyze', 'them', 'in', 'the', 'cloud', '.', 'data', 'science', 'for', 'internet', 'of', 'things', '(', 'iot', ')', ':', 'data', 'science', 'sits', 'at', 'the', 'core', 'of', 'internet', 'of', 'things', 'to', 'makes', 'the', 'things', 'smart', 'as', 'well', 'as', 'capture', 'insights', 'from', 'the', 'connected', 'things', '(', 'sensors', ',', 'actuator', ',', 'and', 'machines', ')', '.', 'the', 'methodology', 'for', 'data', 'science', 'implementations', 'in', 'an', 'iot', 'environment', 'is', 'different', 'from', 'traditional', 'techniques', '.', 'as', 'per', 'gartner', ',', 'there', 'would', 'be', '21', 'billion', 'connected', 'iot', 'devices', 'which', 'makes', 'it', 'essential', 'for', 'a', 'data', 'scientist', 'to', 'learn', 'the', 'applications', 'of', 'data', 'science', 'in', 'iot', 'environments', '.', 'natural', 'language', 'processing', '(', 'nlp', ')', 'is', 'the', 'ability', 'to', 'comprehend', 'human', 'language', '.', 'this', 'may', 'be', 'written', 'text', ',', 'speech', ',', 'or', 'video', '.', 'the', 'natural', 'language', 'doesn', '’', 't', 'have', 'any', 'fixed', 'structure', '.', 'this', 'makes', 'it', 'difficult', 'to', 'store', 'and', 'process', 'data', '.', 'nlp', 'is', 'a', 'current', 'hot', 'topic', ',', 'and', 'we', 'can', 'find', 'several', 'solutions', 'revolving', 'in', 'this', 'space', '.', 'however', ',', 'there', 'is', 'still', 'a', 'lot', 'more', 'to', 'achieve', 'and', 'it', 'is', 'a', 'very', 'active', 'area', 'of', 'research', '.', 'natural', 'language', 'generation', '(', 'nlg', ')', 'turns', 'raw', 'data', 'into', 'a', 'language', 'that', 'any', 'audience', 'with', 'no', 'knowledge', 'of', 'the', 'raw', 'data', 'can', 'understand', '.', 'the', 'simplest', 'level', 'of', 'nlg', 'is', 'to', 'turn', 'a', 'few', 'data', 'points', 'to', 'sentences', '.', 'this', 'is', 'a', 'niche', 'area', ',', 'and', 'data', 'scientists', 'should', 'learn', 'the', 'approach', 'and', 'techniques', 'to', 'embed', 'nlg', 'in', 'analytics', 'systems', '.', 'deep', 'learning', 'is', 'a', 'fast-growing', 'field', 'in', 'data', 'science', '.', 'the', 'ability', 'of', 'deep', 'learning', 'methods', 'to', 'learn', 'complex', 'nonlinear', 'relations', 'makes', 'it', 'stand', 'out', 'from', 'the', 'traditional', 'machine', 'learning', 'techniques', '.', 'neural', 'networks', 'are', 'the', 'precursors', 'to', 'deep', 'learning', '.', 'what', 'makes', 'deep', 'learning', 'different', 'is', 'the', 'use', 'of', 'a', 'high', 'number', 'of', 'hidden', 'layers', 'which', 'have', 'been', 'possible', 'due', 'to', 'the', 'growth', 'of', 'computation', 'power', '.', 'tensorflow', 'and', 'h2o.ai', '’', 's', 'deep', 'learning', 'packages', 'are', 'open', 'source', 'and', 'provide', 'a', 'good', 'platform', 'to', 'start', 'implementing', 'deep', 'learning', 'algorithms', '.', 'reinforcement', 'learning', ':', 'in', 'this', 'learning', 'method', ',', 'a', 'system', 'automatically', 'tries', 'to', 'understand', 'the', 'situation', ',', 'learn', 'from', 'the', 'interactions', ',', 'and', 'choose', 'the', 'optimal', 'path', 'for', 'itself', 'to', 'attend', 'its', 'objective', '.', 'this', 'is', 'based', 'on', 'a', 'reward', 'system', 'where', 'the', 'learner', 'is', 'not', 'told', 'what', 'action', 'to', 'take', 'but', 'rewarded', 'when', 'it', 'takes', 'the', 'correct', 'decision', '.', 'this', 'method', 'is', 'same', 'as', 'how', 'a', 'student', 'learns', '—', 'rewarded', 'when', 'he', 'excels', 'in', 'the', 'exam', 'and', 'punished', 'when', 'he', 'fails', '.', 'this', 'is', 'a', 'niche', 'area', 'for', 'data', 'scientists', 'to', 'explore', 'and', 'contribute', '.', 'you', 'can', 'learn', 'more', 'about', 'reinforcement', 'learning', 'from', 'the', 'mit', 'press', 'home', 'page', 'for', 'this', 'book', '.', 'transfer', 'learning', 'is', 'a', 'current', 'hot', 'topic', 'to', 'be', 'explored', 'by', 'data', 'scientists', '.', 'in', 'this', 'method', ',', 'a', 'new', 'task', 'can', 'be', 'learned', 'by', 'transferring', 'the', 'knowledge', 'from', 'a', 'related', 'task', 'that', 'has', 'already', 'been', 'learned', '.', 'this', 'has', 'immense', 'application', 'for', 'reusing', 'models', 'across', 'domains', ',', 'in', 'areas', 'where', 'the', 'data', 'is', 'sparse', '.', 'as', 'an', 'example', ',', 'using', 'transfer', 'learning', ',', 'one', 'can', 'develop', 'a', 'sentiment', 'analysis', 'model', 'on', 'some', 'products', 'where', 'abundant', 'reviews', 'are', 'available', 'and', 'use', 'this', 'knowledge', 'to', 'develop', 'the', 'same', 'type', 'of', 'model', 'for', 'some', 'other', 'products', 'with', 'sparse', 'reviews', '.', 'you', 'can', 'learn', 'more', 'about', 'this', 'technique', 'from', 'the', 'transfer', 'learning', 'handbook', 'from', 'university', 'of', 'wisconsin', 'by', 'lisa', 'torrey', 'and', 'jude', 'shavlik', '.', 'data', 'science', 'automation', ':', 'innovation', 'for', 'data', 'science', 'automation', 'has', 'begun', 'and', 'will', 'evolve', 'gradually', 'in', 'years', 'to', 'come', '.', 'we', 'are', 'currently', 'at', 'a', 'stage', 'where', 'we', 'have', 'begun', 'to', 'tackle', 'automation', 'for', 'individual', 'data', 'science', 'modules', '.', 'from', 'here', ',', 'we', 'need', 'to', 'move', 'to', 'a', 'more', 'generic', 'data', 'science', 'platform', ',', 'with', 'all', 'modules', 'automated', 'and', 'integrated', 'together', '.', 'i', 'had', 'provided', 'an', 'overview', 'on', 'this', 'in', 'data', 'science', 'automation', 'for', 'big', 'data', 'and', 'iot', 'environment', '.', 'there', 'are', 'never', 'ending', 'discussions', 'on', 'data', 'scientists', 'automated', 'and', 'unemployed', 'by', '2025', '!', 'there', 'is', 'a', 'threat', 'for', 'new', 'data', 'scientists', 'and', 'emerging', 'data', 'technologies', 'professional', 'to', 'be', 'brought', 'down', 'by', 'some', 'other', 'data', 'x', 'in', 'future', '.', 'it', 'is', 'important', 'to', 'remain', 'updated', 'and', 'sharpen', 'skill', 'sets', 'with', 'time', '.', 'technologies', 'are', 'changing', 'at', 'a', 'breakneck', 'pace', 'than', 'the', 'fast', 'paced', 'living', 'in', 'new', 'york', 'and', 'london', '.', 'the', 'best', 'plan', 'is', 'to', 'remain', 'a', 'data', 'lover', 'and', 'be', 'data', 'magicians', '.', 'let', 'the', 'job', 'role', 'and', 'title', 'takes', 'its', 'own', 'route', 'along', 'with', 'the', 'trend', '.', '©', '2020', 'github', ',', 'inc', '.'], ['migrating', 'a', 'mysql', 'database', 'usually', 'requires', 'only', 'a', 'few', 'simple', 'steps', ',', 'but', 'can', 'take', 'quite', 'some', 'time', ',', 'depending', 'on', 'the', 'amount', 'of', 'data', 'you', 'would', 'like', 'to', 'migrate', '.', 'the', 'following', 'steps', 'will', 'guide', 'you', 'through', 'how', 'to', 'export', 'a', 'mysql', 'database', 'from', 'the', 'old', 'server', ',', 'secure', 'it', ',', 'copy', 'it', 'to', 'the', 'new', 'server', ',', 'import', 'it', 'successfully', ',', 'and', 'make', 'sure', 'the', 'data', 'is', 'there', '.', 'exporting', 'mysql', 'database', 'to', 'a', 'dump', 'file', 'oracle', 'provides', 'a', 'utility', 'named', 'mysqldump', 'that', 'allows', 'you', 'to', 'easily', 'export', 'the', 'database', 'structure', 'and', 'data', 'to', 'an', 'sql', 'dump', 'file', '.', 'use', 'the', 'following', 'command', ':', 'mysqldump', '-u', 'root', '-p', '--', 'opt', '[', 'database', 'name', ']', '>', '[', 'database', 'name', ']', '.sql', 'a', 'few', 'notes', ':', 'we', \"'re\", 'using', 'the', '--', 'single-transaction', 'flag', 'to', 'avoid', 'a', 'database', 'lock', 'while', 'exporting', 'the', 'data', '.', 'it', 'will', 'allow', 'you', 'to', 'continue', 'updating', 'data', 'in', 'your', 'old', 'database', 'while', 'exporting', 'the', 'dump', 'file', '.', 'please', 'note', ',', 'though', ',', 'that', 'new', 'data', 'that', 'will', 'be', 'updated', 'after', 'the', 'exporting', 'process', 'already', 'started', '—', 'it', 'wo', \"n't\", 'be', 'available', 'in', 'the', 'exported', 'dump', 'file', '.', 'make', 'sure', 'to', 'replace', '[', 'database', 'name', ']', 'with', 'your', 'actual', 'database', 'name', 'before', 'running', 'the', 'command', '.', 'make', 'sure', 'to', 'enter', 'your', 'user', 'credentials', 'instead', 'of', 'user', 'and', 'password', 'in', 'the', 'command', '.', 'make', 'sure', 'the', 'user', 'has', 'permissions', 'to', 'back', 'up', 'the', 'database', '.', 'secure', 'the', 'backup', 'file', 'in', 'most', 'cases', ',', 'an', 'organization', \"'s\", 'data', 'is', 'its', 'most', 'critical', 'asset', '.', 'therefore', ',', 'we', 'do', 'not', 'want', 'database', 'backups', 'laying', 'around', 'in', 'our', 'servers', 'unprotected', ',', 'as', 'they', 'can', 'mistakenly', 'leak', 'or', ',', 'even', 'worse', ',', 'get', 'stolen', 'by', 'hackers', '.', 'therefore', ',', 'at', 'the', 'first', 'chance', 'you', 'get', ',', 'compress', 'and', 'encrypt', 'the', 'file', 'and', 'delete', 'the', 'original', 'file', '.', 'to', 'encrypt', 'the', 'file', 'to', 'a', 'compressed', 'file', 'in', 'linux', 'operating', 'systems', ',', 'use', 'this', 'command', ':', 'zip', '--', 'encrypt', 'dump.zip', 'db.sql', 'you', 'will', 'be', 'prompted', 'to', 'enter', 'a', 'password', 'before', 'the', 'compression', 'starts', '.', 'transfer', 'the', 'backup', 'file', 'now', 'that', 'we', 'have', 'an', 'encrypted', 'dump', 'file', ',', 'let', \"'s\", 'transfer', 'it', 'over', 'the', 'network', 'to', 'the', 'new', 'server', ',', 'using', 'scp', ':', 'scp', '/path/to/source-file', 'user', '@', 'host', ':', '/path/to/destination-folder/', 'import', 'mysql', 'dump', 'to', 'new', 'server', 'now', 'that', 'we', 'have', 'the', 'backup', 'file', 'on', 'the', 'new', 'server', ',', 'let', \"'s\", 'decrypt', 'and', 'extract', 'it', ':', 'unzip', '-p', 'your-password', 'dump.zip', 'once', 'the', 'file', 'is', 'imported', ',', 'remember', 'to', 'delete', 'the', 'dump', 'file', 'both', 'for', 'storage', 'and', 'security', 'reasons', '.', 'to', 'import', 'the', 'file', ',', 'use', 'the', 'following', 'command', ':', 'mysql', '-u', 'root', '-p', 'newdatabase', '<', '/path/to/newdatabase.sql', 'validate', 'imported', 'data', 'in', 'new', 'server', 'now', 'that', 'we', 'have', 'the', 'database', 'imported', 'on', 'the', 'new', 'server', ',', 'we', 'need', 'a', 'way', 'to', 'make', 'sure', 'that', 'the', 'data', 'is', 'actually', 'there', 'and', 'that', 'we', 'did', \"n't\", 'lose', 'anything', '.', 'we', 'recommend', 'to', 'start', 'with', 'running', 'this', 'query', 'on', 'both', 'the', 'old', 'and', 'new', 'databases', 'and', 'compare', 'the', 'results', '.', 'the', 'query', 'will', 'count', 'the', 'rows', 'on', 'all', 'tables', ',', 'which', 'will', 'provide', 'an', 'indication', 'of', 'the', 'amount', 'of', 'data', 'in', 'both', 'databases', '.', 'select', 'table_name', ',', 'table_rows', 'from', '`', 'information_schema', '`', '.', '`', 'tables', '`', 'where', '`', 'table_schema', '`', '=', \"'your_db_name\", \"'\", ';', 'in', 'addition', ',', 'we', 'recommend', 'to', 'check', 'for', 'min', 'and', 'max', 'records', 'of', 'numeric', 'columns', 'in', 'the', 'tables', 'to', 'make', 'sure', 'the', 'data', 'itself', 'is', 'valid', 'and', 'not', 'only', 'the', 'amount', 'of', 'data', '(', 'this', 'is', 'only', 'an', 'indication', 'though', ')', '.', 'another', 'option', 'for', 'a', 'test', 'is', 'to', 'export', 'the', 'database', 'from', 'the', 'new', 'server', 'and', 'compare', 'the', 'sql', 'dump', 'with', 'the', 'old', 'server', \"'s\", 'sql', 'dump', '.', 'also', ',', 'before', 'migrating', 'the', 'application', 'itself', ',', 'we', 'recommend', 'to', 'redirect', 'one', 'application', 'instance', 'to', 'the', 'new', 'database', 'and', 'confirm', 'that', 'everything', 'is', 'working', 'properly', '.', 'another', 'export', 'and', 'import', 'option', 'we', 'kept', 'this', 'option', 'at', 'the', 'end', ',', 'as', 'we', 'do', 'not', 'really', 'recommend', 'working', 'with', 'it', '.', 'it', 'seems', 'to', 'be', 'a', 'lot', 'easier', ',', 'as', 'it', 'will', 'export', 'and', 'transfer', 'the', 'dump', 'file', 'and', 'import', 'the', 'data', 'to', 'the', 'new', 'database', ',', 'all', 'in', 'one', 'command', '.', 'the', 'downside', ',', 'though', ',', 'is', 'that', 'if', 'the', 'network', 'link', 'dies', ',', 'you', 'need', 'to', 'start', 'over', '.', 'therefore', ',', 'we', 'believe', 'it', \"'s\", 'less', 'recommended', 'to', 'work', 'with', 'this', 'command', '—', 'especially', 'with', 'a', 'large', 'database', '.', 'if', 'you', 'would', 'like', 'to', 'try', 'it', 'anyway', ',', 'use', 'this', 'command', ':', 'mysqldump', '-u', 'root', '-ppassword', '--', 'all-databases', '|', 'ssh', 'user', '@', 'new_host.host.com', \"'cat\", '-', '|', 'mysql', '-u', 'root', \"-ppassword'\", 'important', 'notes', 'make', 'sure', 'have', 'both', 'mysql', 'servers', 'installed', 'with', 'the', 'same', 'official', 'distribution', 'and', 'version', '.', 'otherwise', ',', 'you', \"'ll\", 'need', 'to', 'follow', 'the', 'upgrade', 'instructions', 'from', 'mysql', \"'s\", 'website', '.', 'make', 'sure', 'you', 'have', 'enough', 'space', 'in', 'your', 'old', 'server', 'to', 'hold', 'the', 'dump', 'file', 'and', 'the', 'compressed', 'file', '(', '2', 'x', 'db_size', 'should', 'be', 'available', ')', '.', 'make', 'sure', 'you', 'have', 'enough', 'space', 'in', 'your', 'new', 'server', 'to', 'hold', 'the', 'encrypted', 'dump', 'file', ',', 'the', 'decrypted', 'dump', 'file', ',', 'and', 'the', 'imported', 'database', '(', '3', 'x', 'db_size', 'should', 'be', 'available', ')', '.', 'if', 'you', 'ever', 'considered', 'just', 'moving', 'the', 'datadir', 'from', 'one', 'database', 'to', 'another', ',', 'please', 'do', \"n't\", '.', 'you', 'do', 'not', 'want', 'to', 'mess', 'with', 'the', 'internal', 'structure', 'of', 'the', 'database', ',', 'as', 'it', \"'s\", 'very', 'likely', 'to', 'be', 'an', 'invitation', 'for', 'trouble', 'in', 'the', 'future', '.', 'do', 'not', 'forget', 'to', 'configure', 'important', 'flags', 'such', 'as', 'innodb_log_file_size', 'in', 'your', 'new', 'server', \"'s\", 'configurations', '.', 'forgetting', 'to', 'update', 'the', 'configuration', 'according', 'to', 'the', 'new', 'server', \"'s\", 'specifications', 'might', 'result', 'in', 'serious', 'performance', 'issues', '.', 'in', 'many', 'cases', ',', 'the', 'incentive', 'to', 'upgrade', 'to', 'a', 'new', 'database', 'server', 'is', 'to', 'improve', 'query', 'performance', '.', 'if', 'the', 'upgrade', 'did', \"n't\", 'come', 'with', 'the', 'expected', 'improvement', ',', 'you', 'probably', 'should', 'look', 'into', 'optimizing', 'your', 'sql', 'queries', 'and', 'not', 'only', 'upgrading', 'your', 'hardware', '.', 'enjoy', 'your', 'new', 'server', '!'], ['a', 'graph', 'database', 'is', 'a', 'data', 'management', 'system', 'software', '.', 'the', 'building', 'blocks', 'are', 'vertices', 'and', 'edges', '.', 'to', 'put', 'it', 'in', 'a', 'more', 'familiar', 'context', ',', 'a', 'relational', 'database', 'is', 'also', 'a', 'data', 'management', 'software', 'in', 'which', 'the', 'building', 'blocks', 'are', 'tables', '.', 'both', 'require', 'loading', 'data', 'into', 'the', 'software', 'and', 'using', 'a', 'query', 'language', 'or', 'apis', 'to', 'access', 'the', 'data', '.', 'relational', 'databases', 'boomed', 'in', 'the', '1980s', '.', 'many', 'commercial', 'companies', '(', 'i.e', '.', 'oracle', ',', 'ingres', ',', 'ibm', ')', 'backed', 'the', 'relational', 'model', '(', 'tabular', 'organization', ')', 'of', 'data', 'management', '.', 'in', 'that', 'era', ',', 'the', 'main', 'data', 'management', 'need', 'was', 'to', 'generate', 'reports', '.', 'graph', 'databases', 'did', \"n't\", 'see', 'a', 'greater', 'advantage', 'over', 'relational', 'databases', 'until', 'recent', 'years', ',', 'when', 'frequent', 'schema', 'changes', ',', 'managing', 'explosives', 'volume', 'of', 'data', ',', 'real-time', 'query', 'response', 'time', ',', 'and', 'more', 'intelligent', 'data', 'activation', 'requirements', 'make', 'people', 'realize', 'the', 'advantages', 'of', 'the', 'graph', 'model', '.', 'there', 'are', 'commercial', 'software', 'companies', 'backing', 'this', 'model', 'for', 'many', 'years', ',', 'including', 'tigergraph', '(', 'formerly', 'named', 'graphsql', ')', ',', 'neo4j', ',', 'and', 'datastax', '.', 'the', 'technology', 'is', 'disrupting', 'many', 'areas', ',', 'such', 'as', 'supply', 'chain', 'management', ',', 'e-commerce', 'recommendations', ',', 'security', ',', 'fraud', 'detection', ',', 'and', 'many', 'other', 'areas', 'in', 'advanced', 'data', 'analytics', '.', 'here', ',', 'we', 'discuss', 'the', 'major', 'advantages', 'of', 'using', 'graph', 'databases', 'from', 'a', 'data', 'management', 'point', 'of', 'view', '.', 'object-oriented', 'thinking', 'this', 'means', 'very', 'clear', ',', 'explicit', 'semantics', 'for', 'each', 'query', 'you', 'write', '.', 'there', 'are', 'no', 'hidden', 'assumptions', ',', 'such', 'as', 'relational', 'sql', 'where', 'you', 'have', 'to', 'know', 'how', 'the', 'tables', 'in', 'the', 'from', 'clause', 'will', 'implicitly', 'form', 'cartesian', 'products', '.', 'performance', 'they', 'have', 'superior', 'performance', 'for', 'querying', 'related', 'data', ',', 'big', 'or', 'small', '.', 'a', 'graph', 'is', 'essentially', 'an', 'index', 'data', 'structure', '.', 'it', 'never', 'needs', 'to', 'load', 'or', 'touch', 'unrelated', 'data', 'for', 'a', 'given', 'query', '.', 'they', \"'re\", 'an', 'excellent', 'solution', 'for', 'real-time', 'big', 'data', 'analytical', 'queries', '.', 'better', 'problem-solving', 'graph', 'databases', 'solve', 'problems', 'that', 'are', 'both', 'impractical', 'and', 'practical', 'for', 'relational', 'queries', '.', 'examples', 'include', 'iterative', 'algorithms', 'such', 'as', 'pagerank', ',', 'gradient', 'descent', ',', 'and', 'other', 'data', 'mining', 'and', 'machine', 'learning', 'algorithms', '.', 'research', 'has', 'proved', 'that', 'some', 'graph', 'query', 'languages', 'are', 'turing', 'complete', ',', 'meaning', 'that', 'you', 'can', 'write', 'any', 'algorithm', 'on', 'them', '.', 'there', 'are', 'many', 'query', 'languages', 'in', 'the', 'market', 'that', 'have', 'limited', 'expressive', 'power', ',', 'though', '.', 'make', 'sure', 'you', 'ask', 'many', 'hypothetical', 'questions', 'to', 'see', 'if', 'it', 'can', 'answer', 'them', 'before', 'you', 'lock', 'in', '.', 'update', 'data', 'in', 'real-time', 'and', 'support', 'queries', 'simultaneously', 'graph', 'databases', 'can', 'perform', 'real-time', 'updates', 'on', 'big', 'data', 'while', 'supporting', 'queries', 'at', 'the', 'same', 'time', '.', 'this', 'is', 'a', 'major', 'drawback', 'of', 'existing', 'big', 'data', 'management', 'systems', 'such', 'as', 'hadoop', 'hdfs', 'since', 'it', 'was', 'designed', 'for', 'data', 'lakes', ',', 'where', 'sequential', 'scans', 'and', 'appending', 'new', 'data', '(', 'no', 'random', 'seek', ')', 'are', 'the', 'characteristics', 'of', 'the', 'intended', 'workload', ',', 'and', 'it', 'is', 'an', 'architecture', 'design', 'choice', 'to', 'ensure', 'fast', 'scan', 'i/o', 'of', 'an', 'entire', 'file', '.', 'the', 'assumption', 'there', 'was', 'that', 'any', 'query', 'will', 'touch', 'the', 'majority', 'of', 'a', 'file', ',', 'while', 'graph', 'databases', 'only', 'touch', 'relevant', 'data', ',', 'so', 'a', 'sequential', 'scan', 'is', 'not', 'an', 'optimization', 'assumption', '.', 'flexible', 'online', 'schema', 'environment', 'graph', 'databases', 'offer', 'a', 'flexible', 'online', 'schema', 'evolvement', 'while', 'serving', 'your', 'query', '.', 'you', 'can', 'constantly', 'add', 'and', 'drop', 'new', 'vertex', 'or', 'edge', 'types', 'or', 'their', 'attributes', 'to', 'extend', 'or', 'shrink', 'your', 'data', 'model', '.', 'it', \"'s\", 'so', 'convenient', 'to', 'manage', 'explosive', 'and', 'constantly', 'changing', 'object', 'types', '.', 'the', 'relational', 'database', 'just', 'can', 'not', 'easily', 'adapt', 'to', 'this', 'requirement', ',', 'which', 'is', 'commonplace', 'in', 'the', 'modern', 'data', 'management', 'era', '.', 'group', 'by', 'aggregate', 'queries', 'graph', 'databases', ',', 'in', 'addition', 'to', 'traditional', 'group-by', 'queries', ',', 'can', 'do', 'certain', 'classes', 'of', 'group', 'by', 'aggregate', 'queries', 'that', 'are', 'unimaginable', 'or', 'impractical', 'in', 'relational', 'databases', '.', 'due', 'to', 'the', 'tabular', 'model', 'restriction', ',', 'aggregate', 'queries', 'on', 'a', 'relational', 'database', 'are', 'greatly', 'constrained', 'by', 'how', 'data', 'is', 'grouped', 'together', '.', 'in', 'contrast', ',', 'graph', 'models', 'are', 'more', 'flexible', 'for', 'grouping', 'and', 'aggregating', 'relevant', 'data', '.', 'see', 'this', 'article', 'on', 'the', 'latest', 'expressive', 'power', 'of', 'aggregation', 'for', 'graph', 'traversal', '.', 'i', 'don', '’', 't', 'think', 'relational', 'databases', 'can', 'do', 'this', 'kind', 'of', 'flexible', 'aggregation', 'on', 'selective', 'data', 'points', '.', '(', 'disclaimer', ':', 'i', 'have', 'worked', 'on', 'commercial', 'relational', 'database', 'kernels', 'for', 'a', 'decade', ';', 'oracle', ',', 'ms', 'sql', 'server', ',', 'apache', 'popular', 'open-source', 'platforms', ',', 'etc', '.', ')', 'combine', 'and', 'hierarchize', 'multiple', 'dimensions', 'graph', 'databases', 'can', 'combine', 'multiple', 'dimensions', 'to', 'manage', 'big', 'data', ',', 'including', 'time', 'series', ',', 'demographic', ',', 'geo-dimensions', ',', 'etc', '.', 'with', 'a', 'hierarchy', 'of', 'granularity', 'on', 'different', 'dimensions', '.', 'think', 'about', 'an', 'application', 'in', 'which', 'we', 'want', 'to', 'segment', 'a', 'group', 'of', 'a', 'population', 'based', 'on', 'both', 'time', 'and', 'geo', 'dimensions', '.', 'with', 'a', 'carefully', 'designed', 'graph', 'schema', ',', 'data', 'scientists', 'and', 'business', 'analysts', 'can', 'conduct', 'virtually', 'any', 'analytical', 'query', 'on', 'a', 'graph', 'database', '.', 'this', 'capability', 'traditionally', 'is', 'only', 'accessible', 'to', 'low-level', 'programming', 'languages', 'such', 'as', 'c++', 'and', 'java', '.', 'ai', 'infrastructure', 'graph', 'databases', 'serve', 'as', 'great', 'ai', 'infrastructure', 'due', 'to', 'well-structured', 'relational', 'information', 'between', 'entities', ',', 'which', 'allows', 'one', 'to', 'further', 'infer', 'indirect', 'facts', 'and', 'knowledge', '.', 'machine', 'learning', 'experts', 'love', 'them', '.', 'they', 'provide', 'rich', 'information', 'and', 'convenient', 'data', 'accessibility', 'that', 'other', 'data', 'models', 'can', 'hardly', 'satisfy', '.', 'for', 'example', ',', 'the', 'google', 'expander', 'team', 'has', 'used', 'it', 'for', 'smart', 'messaging', 'technology', '.', 'the', 'knowledge', 'graph', 'was', 'created', 'by', 'google', 'to', 'understand', 'humans', 'better', ',', 'and', 'many', 'more', 'advances', 'are', 'being', 'made', 'on', 'knowledge', 'inference', '.', 'the', 'keys', 'of', 'a', 'successful', 'graph', 'database', 'to', 'serve', 'as', 'a', 'real-time', 'ai', 'data', 'infrastructure', 'are', ':', 'support', 'for', 'real-time', 'updates', 'as', 'fresh', 'data', 'streams', 'in', 'a', 'highly', 'expressive', 'and', 'user-friendly', 'declarative', 'query', 'language', 'to', 'give', 'full', 'control', 'to', 'data', 'scientists', 'support', 'for', 'deep-link', 'traversal', '(', '>', '3', 'hops', ')', 'in', 'real-time', '(', 'sub-second', ')', ',', 'just', 'like', 'human', 'neurons', 'sending', 'information', 'over', 'a', 'neural', 'network', ';', 'deep', 'and', 'efficient', 'scale', 'out', 'and', 'scale', 'up', 'to', 'manage', 'big', 'graphs', 'in', 'conclusion', ',', 'we', 'see', 'many', 'advantages', 'of', 'native', 'graph', 'databases', 'managing', 'big', 'data', 'that', 'can', 'not', 'be', 'worked', 'around', 'by', 'traditional', 'relational', 'databases', '.', 'however', ',', 'as', 'any', 'new', 'technology', 'replacing', 'old', 'technology', ',', 'there', 'are', 'still', 'obstacles', 'in', 'adopting', 'graph', 'databases', '.', 'one', 'is', 'that', 'there', 'are', 'fewer', 'qualified', 'developers', 'in', 'the', 'job', 'market', 'than', 'the', 'sql', 'developers', '.', 'another', 'is', 'the', 'non-standardization', 'of', 'the', 'graph', 'database', 'query', 'language', '.', 'there', \"'s\", 'been', 'a', 'lot', 'of', 'marketing', 'hype', 'and', 'incomplete', 'offerings', 'that', 'have', 'led', 'to', 'subpar', 'performance', 'and', 'subpar', 'usability', ',', 'which', 'slows', 'down', 'graph', 'model', 'adoption', 'in', 'the', 'needed', 'enterprises', '.'], ['when', 'organizations', 'decide', 'to', 'shift', 'their', 'workloads', ',', 'data', 'and', 'processes', 'across', 'multiple', 'on-premises', ',', 'hosted', ',', 'private', ',', 'and', 'public', 'cloud', 'services', ',', 'there', 'will', 'be', 'a', 'need', 'for', 'a', 'new', 'approach', '.', 'this', 'new', 'approach', 'leads', 'to', 'hybrid', 'multi-cloud', 'cloud', 'management', '.', 'but', 'this', 'approach', 'requires', 'uniform', 'solutions', 'in', 'terms', 'of', 'billing', 'and', 'provisioning', ',', 'access', 'control', ',', 'cost', 'control', ',', 'and', 'performance', 'analysis', 'and', 'capacity', 'management', '.', 'a', 'hybrid', 'multi-cloud', 'architecture', 'is', 'emerging', 'within', 'nearly', 'all', 'enterprises', '.', 'it', 'organizations', 'are', 'no', 'longer', 'limited', 'to', 'managing', 'data-centers', 'and', 'a', 'few', 'hosted', 'and', 'managed', 'services', 'providers', '.', 'needy', 'lines-of-business', 'teams', 'and', 'impatient', 'it', 'developers', 'have', 'procured', 'saas', ',', 'iaas', ',', 'and', 'paas', 'cloud', 'services', 'to', 'overcome', 'resource', 'constraints', '.', 'now', 'many', 'enterprises', \"'\", 'it', 'structures', 'are', 'composed', 'of', 'multi-clouds', '.', 'in', 'the', 'it', 'industry', ',', 'the', 'tools', 'and', 'technologies', 'needed', 'to', 'craft', 'and', 'manage', 'hybrid', 'multi-clouds', 'architecture', 'are', 'fragmented', '.', 'multi-clouds', 'and', 'hybrid', 'clouds', 'bring', 'workload', 'and', 'infrastructure', 'challenges', 'that', 'will', 'drive', 'the', 'development', 'of', 'new', 'cloud', 'management', 'technology', '.', 'in', 'addition', 'to', 'having', 'to', 'manage', 'resource', 'utilization', ',', 'performance', 'and', 'costs', 'of', 'various', 'public', 'and', 'private', 'cloud', 'services', ',', 'cloud', 'management', 'platforms', 'must', 'also', 'be', 'aware', 'of', 'the', 'integrations', 'and', 'processes', 'that', 'transcend', 'on-premises', 'and', 'cloud', 'execution', 'venues', ',', 'and', 'interoperate', '(', 'in', 'some', 'way', ')', 'with', 'the', 'new', 'multi-purpose', 'hybrid', 'ipaas', 'that', 'connects', 'them', ',', 'to', 'assure', 'business', 'continuity', '.', 'organizations', 'plan', 'to', 'migrate', 'their', 'on-premise', 'systems', 'to', 'hybrid', 'multi-cloud', 'when', 'they', 'need', 'a', 'solution', 'for', 'the', 'following', 'challenges', 'and', 'requirements', ':', 'users', 'are', 'widely', 'distributed', 'geographically', 'where', 'they', 'are', 'surrounded', 'by', 'multiple', 'data', 'centers', 'instead', 'of', 'a', 'single', 'data', 'center', '.', 'facing', 'regulations', 'limit', 'in', 'particular', 'countries', 'for', 'storing', 'data', ',', 'e.g.', ',', 'eu', '.', 'an', 'environment', 'where', 'public', 'clouds', 'are', 'used', 'with', 'on-premises', 'resources', '.', 'a', 'cloud-based', 'application', 'is', 'not', 'resilient', ',', 'which', 'can', 'affect', 'disaster', 'recovery', 'when', 'loss', 'of', 'a', 'single', 'data', 'center', '.', 'according', 'to', 'the', 'above', 'challenges', ',', 'i', '’', 've', 'introduced', 'two', 'hybrid', 'multi-cloud', 'architectures', 'for', 'migrating', 'on-premise', 'environment', 'to', 'a', 'hybrid', 'multi-cloud', 'environment', '.', 'there', 'are', 'many', 'multi-cloud', 'architectures', ',', 'namely', 're-deployment', ',', 'cloudification', ',', 'relocation', ',', 'refactoring', ',', 'rebinding', ',', 'replacement', ',', 'and', 'modernization', 'for', 'organizations', 'to', 'for', 'adopt', 'multi-cloud', 'environments', '.', '1', '.', 'multi-application', 'rebinding', 'image', 'title', 'in', 'the', 'above', 'hybrid', 'multi-cloud', 'architecture', ',', 'a', 're-architected', 'application', 'is', 'deployed', 'partially', 'on', 'multiple', 'cloud', 'environments', '.', 'this', 'architecture', 'can', 'be', 'used', 'for', 'the', 'systems', 'that', 'route', 'users', 'to', 'the', 'nearest', 'data', 'center', 'when', 'the', 'primary', 'or', 'on-premise', 'data', 'center', 'fails', '.', 'in', 'particular', ',', 'they', 'can', 'be', 'configured', 'to', 'monitor', 'the', 'status', 'of', 'the', 'service', 'to', 'which', 'they', 'are', 'directing', 'the', 'users', '.', 'if', 'any', 'service', 'is', 'not', 'available', ',', 'all', 'the', 'traffic', 'will', 'be', 'routed', 'to', 'another', 'healthy', 'instance', '.', 'this', 'architecture', 'uses', 'an', 'on-premise', 'cloud', 'adapter', '(', 'e.g.', ',', 'service', 'bus', 'or', 'elastic', 'load', 'balancer', ')', 'to', 'provide', 'an', 'integration', 'of', 'components', 'in', 'different', 'cloud', 'platforms', '.', 'let', '’', 's', 'understand', 'with', 'an', 'example', '.', 'here', ',', 'ac1', 'and', 'ac2', 'are', 'two', 'application', 'components', 'hosted', 'on-premise', 'before', 'migration', '.', 'as', 'both', 'the', 'components', 'are', 'independent', 'integrity', 'units', ',', 'ac1', 'remains', 'on-premise', 'while', 'two', 'ac2s', 'are', 'deployed', 'on', 'aws', 'and', 'azure', 'for', 'disaster', 'recovery', '.', 'ac1', 'and', 'two', 'ac2', 'components', 'are', 'connected', 'via', 'ebs', 'or', 'service', 'bus', '.', 'the', 'main', 'benefits', 'of', 'using', 'this', 'architecture', 'are', 'the', 'application', '’', 's', 'response', 'rate', 'increases', 'to', 'the', 'maximum', 'level', 'and', 'unhealthy', 'services', 'become', 'healthy', 'again', '.', '2', '.', 'multi-application', 'modernization', 'image', 'title', 'in', 'this', 'architecture', ',', 'on-premise', 'applications', 'are', 're-architected', 'as', 'a', 'portfolio', 'and', 'deployed', 'on', 'the', 'cloud', 'environment', '.', 'in', 'the', 'above', 'example', ',', 'a1', ',', 'a2', ',', 'and', 'one', 'application', 'component', ',', 'ac1', ',', 'are', 're-architected', 'as', 'a', 'portfolio', 'and', 'deployed', 'on', 'different', 'cloud', 'providers', 'and', 'on-premise', '.', 'ac1', 'is', 'deployed', 'on', 'aws', 'and', 'a2', 'is', 'deployed', 'on', 'azure', 'while', 'a1', 'is', 'kept', 'on-premise', '.', 'this', 'architecture', 'overcomes', 'the', 'problem', 'where', 're-architecting', 'an', 'on-premise', 'application', 'does', 'not', 'remove', 'duplicated', 'functionality', 'and', 'inconsistencies', '.', 'multi-application', 'modernization', 'analyzes', 'an', 'application', 'as', 'a', 'portfolio', 'to', 'identify', 'opportunities', 'for', 'consolidation', 'and', 'sharing', '.', 'the', 'separation', 'of', 'workloads', 'enables', 'the', 'identification', 'of', 'components', 'that', 'are', 'shared', 'by', 'more', 'than', 'one', 'solution', '.', 'this', 'architecture', 'provides', 'a', 'consistent', 'performance', 'and', 'reduces', 'operational', 'tasks', 'and', 'maintenance', 'costs', 'for', 'shared', 'components', '.', 'conclusion', 'the', 'introduction', 'of', 'hybrid', 'multi-cloud', 'architectures', 'complements', 'existing', 'migration', 'practices', 'and', 'allows', 'for', 'an', 'engineering', 'approach', 'towards', 'constructing', 'and', 'evaluating', 'the', 'migration', 'plan', '.', 'multi-cloud', 'architectures', 'provide', 'an', 'environment', 'where', 'businesses', 'can', 'build', 'secure', 'and', 'powerful', 'cloud', 'environments', 'outside', 'the', 'traditional', 'infrastructure', '.', 'maximizing', 'the', 'impact', 'of', 'multi-cloud', ',', 'however', ',', 'means', 'tackling', 'the', 'challenges', 'of', 'app', 'sprawl', ',', 'unique', 'portals', ',', 'compliance', ',', 'migration', ',', 'and', 'security', 'head-on', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "gen_docs = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in raw_documents]\n",
    "print(gen_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "dictionary = gensim.corpora.Dictionary(gen_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 2), (2, 2), (3, 27), (4, 49), (5, 7), (6, 1), (7, 1), (8, 43), (9, 1), (10, 1), (11, 1), (12, 1), (13, 5), (14, 6), (15, 5), (16, 4), (17, 7), (18, 1), (19, 1), (20, 1), (21, 29), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 11), (28, 3), (29, 1), (30, 1), (31, 1), (32, 1), (33, 8), (34, 1), (35, 3), (36, 1), (37, 1), (38, 4), (39, 16), (40, 3), (41, 3), (42, 10), (43, 1), (44, 1), (45, 3), (46, 6), (47, 1), (48, 1), (49, 1), (50, 3), (51, 2), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 3), (58, 1), (59, 6), (60, 1), (61, 9), (62, 2), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 2), (80, 1), (81, 1), (82, 1), (83, 1), (84, 2), (85, 1), (86, 1), (87, 1), (88, 1), (89, 1), (90, 1), (91, 1), (92, 1), (93, 3), (94, 1), (95, 1), (96, 1), (97, 2), (98, 53), (99, 1), (100, 1), (101, 3), (102, 1), (103, 1), (104, 1), (105, 3), (106, 2), (107, 1), (108, 1), (109, 1), (110, 1), (111, 1), (112, 3), (113, 1), (114, 3), (115, 1), (116, 2), (117, 2), (118, 1), (119, 1), (120, 3), (121, 1), (122, 1), (123, 1), (124, 1), (125, 1), (126, 1), (127, 1), (128, 1), (129, 1), (130, 2), (131, 1), (132, 1), (133, 1), (134, 1), (135, 4), (136, 2), (137, 1), (138, 1), (139, 13), (140, 1), (141, 1), (142, 1), (143, 1), (144, 1), (145, 1), (146, 1), (147, 7), (148, 1), (149, 1), (150, 1), (151, 1), (152, 1), (153, 1), (154, 3), (155, 2), (156, 1), (157, 1), (158, 2), (159, 1), (160, 1), (161, 1), (162, 4), (163, 4), (164, 1), (165, 1), (166, 1), (167, 1), (168, 1), (169, 1), (170, 3), (171, 2), (172, 1), (173, 1), (174, 1), (175, 7), (176, 1), (177, 1), (178, 1), (179, 2), (180, 1), (181, 1), (182, 1), (183, 2), (184, 1), (185, 1), (186, 1), (187, 1), (188, 1), (189, 1), (190, 1), (191, 1), (192, 4), (193, 1), (194, 1), (195, 1), (196, 1), (197, 2), (198, 14), (199, 10), (200, 2), (201, 2), (202, 2), (203, 3), (204, 4), (205, 2), (206, 1), (207, 14), (208, 1), (209, 1), (210, 1), (211, 1), (212, 3), (213, 1), (214, 15), (215, 1), (216, 1), (217, 1), (218, 1), (219, 1), (220, 1), (221, 2), (222, 2), (223, 2), (224, 1), (225, 1), (226, 2), (227, 1), (228, 1), (229, 2), (230, 1), (231, 1), (232, 1), (233, 1), (234, 1), (235, 1), (236, 4), (237, 5), (238, 3), (239, 1), (240, 1), (241, 2), (242, 1), (243, 2), (244, 3), (245, 2), (246, 1), (247, 1), (248, 1), (249, 2), (250, 1), (251, 3), (252, 2), (253, 1), (254, 29), (255, 7), (256, 1), (257, 1), (258, 1), (259, 3), (260, 2), (261, 1), (262, 1), (263, 2), (264, 2), (265, 1), (266, 3), (267, 1), (268, 1), (269, 1), (270, 1), (271, 1), (272, 1), (273, 1), (274, 1), (275, 2), (276, 4), (277, 3), (278, 2), (279, 5), (280, 1), (281, 1), (282, 1), (283, 4), (284, 2), (285, 1), (286, 1), (287, 1), (288, 1), (289, 1), (290, 2), (291, 1), (292, 1), (293, 1), (294, 1), (295, 1), (296, 4), (297, 1), (298, 2), (299, 3), (300, 1), (301, 1), (302, 1), (303, 1), (304, 3), (305, 1), (306, 1), (307, 1), (308, 24), (309, 2), (310, 2), (311, 1), (312, 1), (313, 1), (314, 1), (315, 3), (316, 1), (317, 3), (318, 1), (319, 1), (320, 1), (321, 1), (322, 1), (323, 1), (324, 2), (325, 1), (326, 2), (327, 1), (328, 1), (329, 1), (330, 3), (331, 3), (332, 2), (333, 2), (334, 1), (335, 1), (336, 1), (337, 4), (338, 1), (339, 1), (340, 1), (341, 7), (342, 1), (343, 1), (344, 3), (345, 6), (346, 1), (347, 1), (348, 1), (349, 1), (350, 1), (351, 4), (352, 48), (353, 1), (354, 1), (355, 3), (356, 1), (357, 2), (358, 7), (359, 2), (360, 1), (361, 2), (362, 1), (363, 1), (364, 1), (365, 40), (366, 1), (367, 1), (368, 1), (369, 2), (370, 1), (371, 1), (372, 1), (373, 1), (374, 3), (375, 1), (376, 1), (377, 1), (378, 2), (379, 1), (380, 1), (381, 1), (382, 1), (383, 2), (384, 3), (385, 1), (386, 4), (387, 1), (388, 1), (389, 2), (390, 1), (391, 1), (392, 1), (393, 2), (394, 13), (395, 1), (396, 1), (397, 1), (398, 5), (399, 1), (400, 8), (401, 1), (402, 1), (403, 1), (404, 1), (405, 1), (406, 7), (407, 1)], [(1, 5), (2, 5), (3, 41), (4, 58), (5, 7), (6, 1), (8, 30), (10, 1), (14, 2), (15, 2), (17, 5), (18, 1), (21, 46), (23, 2), (27, 6), (28, 4), (29, 1), (33, 8), (35, 5), (37, 1), (39, 2), (40, 1), (42, 4), (43, 2), (45, 1), (46, 6), (48, 3), (49, 2), (50, 1), (51, 12), (54, 1), (57, 2), (58, 2), (59, 6), (61, 6), (67, 1), (69, 1), (71, 1), (73, 1), (84, 1), (88, 1), (96, 2), (97, 1), (98, 61), (100, 1), (101, 6), (105, 2), (106, 1), (117, 3), (118, 1), (122, 1), (123, 1), (124, 1), (134, 1), (137, 1), (139, 18), (147, 11), (155, 1), (156, 1), (158, 1), (160, 1), (162, 3), (163, 6), (166, 1), (168, 3), (169, 1), (170, 1), (175, 31), (179, 1), (181, 1), (183, 1), (184, 1), (186, 2), (189, 2), (192, 2), (197, 5), (198, 22), (199, 10), (200, 4), (201, 1), (203, 3), (204, 5), (205, 7), (206, 2), (207, 15), (212, 1), (214, 3), (215, 1), (221, 6), (226, 1), (233, 3), (235, 1), (236, 2), (237, 1), (238, 2), (239, 4), (240, 1), (241, 1), (243, 3), (244, 1), (246, 1), (247, 1), (248, 3), (249, 3), (251, 1), (252, 1), (254, 28), (255, 6), (259, 1), (260, 1), (261, 3), (263, 3), (265, 1), (273, 2), (274, 1), (275, 1), (279, 4), (280, 2), (284, 2), (285, 1), (290, 2), (296, 1), (297, 2), (299, 1), (301, 2), (308, 24), (309, 2), (310, 10), (313, 1), (317, 1), (318, 1), (320, 1), (322, 1), (323, 1), (324, 3), (326, 1), (327, 1), (329, 1), (332, 1), (333, 1), (341, 3), (342, 1), (343, 1), (344, 2), (346, 1), (347, 6), (351, 3), (352, 54), (354, 6), (355, 2), (356, 4), (358, 18), (361, 3), (365, 38), (366, 1), (368, 1), (377, 1), (378, 3), (383, 1), (384, 2), (388, 1), (390, 1), (392, 2), (394, 7), (395, 1), (396, 2), (398, 4), (400, 3), (404, 1), (406, 11), (407, 1), (408, 1), (409, 1), (410, 1), (411, 1), (412, 1), (413, 1), (414, 1), (415, 1), (416, 1), (417, 1), (418, 1), (419, 2), (420, 2), (421, 1), (422, 2), (423, 1), (424, 1), (425, 1), (426, 1), (427, 1), (428, 1), (429, 1), (430, 1), (431, 1), (432, 7), (433, 2), (434, 1), (435, 1), (436, 1), (437, 1), (438, 1), (439, 1), (440, 1), (441, 1), (442, 1), (443, 1), (444, 1), (445, 1), (446, 1), (447, 1), (448, 1), (449, 1), (450, 5), (451, 1), (452, 1), (453, 1), (454, 1), (455, 2), (456, 1), (457, 1), (458, 1), (459, 2), (460, 1), (461, 1), (462, 1), (463, 2), (464, 1), (465, 1), (466, 1), (467, 1), (468, 1), (469, 1), (470, 1), (471, 1), (472, 1), (473, 1), (474, 1), (475, 1), (476, 1), (477, 1), (478, 1), (479, 3), (480, 1), (481, 1), (482, 2), (483, 1), (484, 1), (485, 2), (486, 1), (487, 1), (488, 1), (489, 1), (490, 1), (491, 1), (492, 1), (493, 1), (494, 3), (495, 1), (496, 1), (497, 4), (498, 1), (499, 1), (500, 1), (501, 1), (502, 1), (503, 1), (504, 2), (505, 1), (506, 3), (507, 2), (508, 1), (509, 1), (510, 1), (511, 2), (512, 1), (513, 1), (514, 1), (515, 2), (516, 1), (517, 1), (518, 2), (519, 1), (520, 2), (521, 1), (522, 1), (523, 1), (524, 1), (525, 1), (526, 1), (527, 1), (528, 1), (529, 1), (530, 2), (531, 1), (532, 1), (533, 2), (534, 1), (535, 1), (536, 1), (537, 1), (538, 1), (539, 1), (540, 1), (541, 1), (542, 2), (543, 2), (544, 1), (545, 1), (546, 1), (547, 1), (548, 1), (549, 1), (550, 1), (551, 1), (552, 1), (553, 1), (554, 1), (555, 1), (556, 1), (557, 1), (558, 1), (559, 1), (560, 1), (561, 1), (562, 6), (563, 2), (564, 1), (565, 1), (566, 1), (567, 1), (568, 2), (569, 1), (570, 3), (571, 1), (572, 1), (573, 1), (574, 1), (575, 1), (576, 1), (577, 1), (578, 1), (579, 1), (580, 1), (581, 2), (582, 1), (583, 1), (584, 1), (585, 1), (586, 3), (587, 1), (588, 1), (589, 1), (590, 1), (591, 1), (592, 2), (593, 1), (594, 1), (595, 1), (596, 1), (597, 1), (598, 1), (599, 2), (600, 2), (601, 3), (602, 1), (603, 2), (604, 1), (605, 2), (606, 1), (607, 1), (608, 2), (609, 1), (610, 1), (611, 1), (612, 2), (613, 1), (614, 1), (615, 1), (616, 2), (617, 1), (618, 1), (619, 1), (620, 1), (621, 1), (622, 1), (623, 1), (624, 2), (625, 1), (626, 3), (627, 2), (628, 1), (629, 1), (630, 1), (631, 3), (632, 1), (633, 1), (634, 1), (635, 1), (636, 1), (637, 1), (638, 1), (639, 1), (640, 1), (641, 2), (642, 1), (643, 1), (644, 6), (645, 1), (646, 1), (647, 1), (648, 1), (649, 1), (650, 4), (651, 2), (652, 1), (653, 1), (654, 1), (655, 1), (656, 1), (657, 2), (658, 1), (659, 1), (660, 2), (661, 3), (662, 1), (663, 1), (664, 2), (665, 1), (666, 1), (667, 1), (668, 1), (669, 1), (670, 1), (671, 1), (672, 1), (673, 1), (674, 1), (675, 1), (676, 1), (677, 1), (678, 1), (679, 4), (680, 1), (681, 1), (682, 1), (683, 1), (684, 1), (685, 1), (686, 1), (687, 3), (688, 1), (689, 2), (690, 1), (691, 1), (692, 2), (693, 2), (694, 2)], [(1, 3), (2, 3), (3, 42), (4, 35), (5, 8), (8, 14), (15, 2), (16, 1), (17, 6), (21, 21), (22, 3), (33, 5), (35, 2), (43, 3), (46, 7), (58, 1), (59, 1), (61, 2), (71, 1), (98, 13), (108, 5), (116, 2), (139, 4), (147, 5), (162, 1), (163, 6), (169, 1), (175, 17), (192, 1), (198, 10), (199, 13), (200, 1), (201, 2), (202, 1), (212, 2), (220, 9), (226, 1), (234, 1), (240, 2), (244, 3), (252, 6), (254, 7), (255, 5), (259, 1), (262, 1), (264, 2), (279, 1), (284, 1), (296, 1), (298, 1), (299, 1), (314, 2), (318, 3), (324, 1), (327, 1), (328, 1), (336, 1), (337, 1), (342, 1), (351, 10), (352, 70), (354, 2), (358, 6), (361, 1), (365, 42), (382, 1), (384, 4), (386, 5), (388, 2), (393, 1), (394, 13), (398, 1), (400, 1), (401, 2), (404, 7), (406, 9), (408, 1), (418, 1), (428, 1), (430, 1), (431, 3), (436, 2), (493, 2), (496, 20), (497, 1), (499, 1), (523, 2), (534, 1), (537, 1), (542, 2), (548, 1), (549, 1), (562, 16), (568, 3), (612, 1), (626, 2), (630, 2), (634, 2), (640, 1), (652, 3), (661, 3), (670, 1), (675, 1), (682, 2), (685, 2), (689, 14), (691, 2), (695, 1), (696, 1), (697, 1), (698, 1), (699, 9), (700, 1), (701, 1), (702, 4), (703, 3), (704, 1), (705, 1), (706, 4), (707, 1), (708, 1), (709, 1), (710, 1), (711, 1), (712, 1), (713, 1), (714, 1), (715, 1), (716, 2), (717, 3), (718, 3), (719, 6), (720, 1), (721, 1), (722, 1), (723, 1), (724, 1), (725, 1), (726, 1), (727, 1), (728, 1), (729, 1), (730, 1), (731, 1), (732, 1), (733, 3), (734, 1), (735, 3), (736, 1), (737, 4), (738, 2), (739, 1), (740, 1), (741, 1), (742, 8), (743, 2), (744, 1), (745, 2), (746, 1), (747, 1), (748, 1), (749, 1), (750, 1), (751, 1), (752, 1), (753, 1), (754, 1), (755, 1), (756, 1), (757, 21), (758, 2), (759, 1), (760, 1), (761, 2), (762, 1), (763, 1), (764, 2), (765, 1), (766, 2), (767, 1), (768, 1), (769, 1), (770, 13), (771, 2), (772, 1), (773, 1), (774, 3), (775, 2), (776, 1), (777, 1), (778, 2), (779, 1), (780, 1), (781, 1), (782, 1), (783, 1), (784, 5), (785, 1), (786, 4), (787, 1), (788, 1), (789, 1), (790, 1), (791, 3), (792, 1), (793, 1), (794, 2), (795, 1), (796, 1), (797, 1), (798, 2), (799, 1), (800, 4), (801, 5), (802, 4), (803, 1), (804, 1), (805, 1), (806, 2), (807, 1), (808, 1), (809, 1), (810, 1), (811, 1), (812, 1), (813, 1), (814, 1), (815, 1), (816, 1), (817, 1), (818, 1), (819, 1), (820, 1), (821, 1), (822, 1), (823, 1), (824, 1), (825, 1), (826, 1), (827, 2), (828, 1), (829, 1), (830, 1), (831, 8), (832, 3), (833, 4), (834, 4), (835, 1), (836, 2), (837, 1), (838, 1), (839, 1), (840, 2), (841, 3), (842, 1), (843, 1), (844, 5), (845, 1), (846, 4), (847, 1), (848, 1), (849, 1), (850, 3), (851, 1), (852, 1), (853, 1), (854, 1), (855, 2), (856, 2), (857, 1), (858, 2), (859, 1), (860, 1), (861, 1), (862, 1), (863, 1), (864, 3), (865, 1), (866, 1), (867, 1), (868, 4), (869, 1), (870, 1), (871, 1), (872, 1), (873, 1), (874, 4), (875, 1), (876, 2), (877, 2), (878, 2), (879, 1), (880, 1), (881, 1), (882, 1), (883, 15), (884, 1), (885, 1), (886, 1), (887, 4), (888, 1), (889, 2), (890, 1), (891, 1), (892, 9), (893, 1), (894, 1), (895, 1), (896, 3), (897, 1), (898, 3), (899, 1), (900, 1), (901, 1), (902, 1), (903, 1), (904, 1), (905, 1), (906, 1), (907, 3), (908, 1), (909, 1), (910, 1), (911, 1), (912, 1), (913, 1), (914, 2), (915, 1), (916, 1), (917, 1), (918, 2), (919, 1), (920, 9), (921, 1), (922, 1), (923, 2)], [(1, 7), (2, 7), (3, 49), (4, 51), (5, 2), (8, 26), (13, 1), (14, 2), (16, 1), (17, 6), (19, 2), (21, 26), (22, 1), (23, 4), (26, 1), (27, 15), (29, 2), (33, 9), (35, 1), (45, 1), (46, 1), (48, 1), (51, 7), (55, 2), (57, 1), (59, 5), (61, 11), (64, 1), (85, 1), (98, 34), (101, 1), (105, 1), (108, 2), (112, 1), (117, 1), (123, 1), (134, 1), (139, 14), (141, 1), (147, 2), (159, 1), (162, 2), (163, 5), (166, 1), (169, 2), (170, 1), (175, 20), (177, 2), (187, 1), (192, 1), (198, 12), (199, 7), (202, 2), (203, 3), (204, 3), (207, 2), (212, 1), (214, 2), (216, 1), (219, 2), (220, 2), (223, 3), (226, 8), (236, 6), (237, 2), (239, 4), (244, 1), (245, 1), (247, 1), (251, 2), (252, 3), (254, 18), (255, 10), (259, 7), (261, 3), (263, 1), (264, 2), (271, 1), (275, 2), (280, 1), (284, 1), (295, 1), (297, 1), (310, 2), (322, 4), (324, 1), (333, 1), (337, 5), (339, 3), (341, 1), (342, 1), (351, 11), (352, 31), (354, 7), (357, 2), (358, 7), (361, 4), (365, 23), (366, 1), (378, 1), (381, 1), (382, 1), (387, 1), (388, 2), (392, 4), (394, 3), (398, 2), (400, 5), (401, 3), (404, 2), (406, 2), (407, 2), (420, 1), (422, 1), (427, 1), (432, 1), (436, 1), (449, 1), (469, 1), (474, 1), (475, 2), (482, 1), (496, 2), (504, 2), (508, 1), (513, 1), (516, 1), (517, 2), (527, 3), (531, 1), (541, 1), (544, 1), (549, 1), (561, 1), (562, 3), (567, 1), (568, 2), (569, 2), (586, 1), (587, 1), (591, 1), (601, 1), (612, 1), (622, 1), (623, 1), (634, 1), (637, 1), (639, 1), (649, 1), (650, 1), (651, 3), (652, 1), (660, 2), (671, 1), (675, 1), (677, 1), (679, 1), (689, 6), (692, 1), (698, 1), (699, 2), (712, 1), (713, 2), (715, 1), (723, 1), (726, 1), (729, 1), (735, 1), (737, 3), (757, 8), (758, 16), (766, 1), (773, 1), (800, 1), (822, 1), (833, 1), (835, 1), (836, 1), (844, 1), (846, 2), (851, 2), (852, 1), (856, 3), (863, 8), (864, 11), (879, 1), (883, 1), (887, 3), (892, 1), (896, 2), (899, 3), (905, 1), (914, 1), (920, 2), (924, 1), (925, 1), (926, 1), (927, 1), (928, 1), (929, 1), (930, 1), (931, 1), (932, 1), (933, 1), (934, 3), (935, 3), (936, 1), (937, 2), (938, 3), (939, 1), (940, 1), (941, 1), (942, 1), (943, 1), (944, 1), (945, 1), (946, 2), (947, 1), (948, 1), (949, 1), (950, 1), (951, 1), (952, 2), (953, 1), (954, 2), (955, 1), (956, 1), (957, 1), (958, 1), (959, 1), (960, 1), (961, 1), (962, 1), (963, 1), (964, 1), (965, 1), (966, 1), (967, 2), (968, 3), (969, 1), (970, 2), (971, 1), (972, 1), (973, 1), (974, 2), (975, 1), (976, 1), (977, 1), (978, 2), (979, 1), (980, 1), (981, 1), (982, 1), (983, 1), (984, 1), (985, 1), (986, 1), (987, 2), (988, 1), (989, 2), (990, 4), (991, 1), (992, 1), (993, 1), (994, 1), (995, 1), (996, 1), (997, 1), (998, 1), (999, 1), (1000, 1), (1001, 1), (1002, 1), (1003, 2), (1004, 1), (1005, 2), (1006, 1), (1007, 1), (1008, 1), (1009, 1), (1010, 1), (1011, 1), (1012, 1), (1013, 1), (1014, 1), (1015, 3), (1016, 1), (1017, 1), (1018, 1), (1019, 1), (1020, 4), (1021, 1), (1022, 1), (1023, 1), (1024, 1), (1025, 1), (1026, 1), (1027, 1), (1028, 1), (1029, 1), (1030, 1), (1031, 1), (1032, 1), (1033, 1), (1034, 23), (1035, 1), (1036, 1), (1037, 1), (1038, 1), (1039, 3), (1040, 1), (1041, 1), (1042, 1), (1043, 1), (1044, 1), (1045, 1), (1046, 1), (1047, 1), (1048, 1), (1049, 1), (1050, 1), (1051, 1), (1052, 1), (1053, 1), (1054, 1), (1055, 1), (1056, 2), (1057, 1), (1058, 1), (1059, 1), (1060, 1), (1061, 1), (1062, 1), (1063, 3), (1064, 1), (1065, 1), (1066, 1), (1067, 1), (1068, 1), (1069, 1), (1070, 1), (1071, 1), (1072, 1), (1073, 3), (1074, 1), (1075, 1), (1076, 1), (1077, 1), (1078, 1), (1079, 1), (1080, 1), (1081, 8), (1082, 2), (1083, 2), (1084, 1), (1085, 1), (1086, 1), (1087, 1), (1088, 1), (1089, 1), (1090, 1), (1091, 2), (1092, 1), (1093, 1), (1094, 1), (1095, 1), (1096, 1), (1097, 1), (1098, 1), (1099, 1), (1100, 1), (1101, 1), (1102, 1), (1103, 1), (1104, 1), (1105, 1), (1106, 1), (1107, 1), (1108, 1), (1109, 1), (1110, 1), (1111, 1), (1112, 1), (1113, 1), (1114, 1), (1115, 1), (1116, 1), (1117, 7), (1118, 1), (1119, 1), (1120, 1), (1121, 13), (1122, 2), (1123, 1), (1124, 1), (1125, 1), (1126, 1), (1127, 1), (1128, 1), (1129, 1), (1130, 1), (1131, 2), (1132, 2), (1133, 1), (1134, 4), (1135, 4), (1136, 1), (1137, 1), (1138, 1), (1139, 1), (1140, 1), (1141, 2), (1142, 1), (1143, 2), (1144, 1), (1145, 1), (1146, 1), (1147, 1), (1148, 1), (1149, 1), (1150, 2), (1151, 1), (1152, 1), (1153, 1), (1154, 2), (1155, 1), (1156, 1), (1157, 1), (1158, 2), (1159, 1), (1160, 4), (1161, 1), (1162, 1), (1163, 3), (1164, 1), (1165, 2), (1166, 1), (1167, 2), (1168, 1), (1169, 1), (1170, 2), (1171, 1), (1172, 1), (1173, 1), (1174, 1), (1175, 1), (1176, 1), (1177, 2), (1178, 1), (1179, 2)], [(1, 2), (2, 2), (3, 44), (4, 38), (5, 1), (8, 14), (15, 2), (16, 1), (17, 8), (18, 1), (21, 37), (22, 1), (23, 1), (26, 9), (27, 16), (33, 4), (43, 1), (46, 5), (57, 1), (58, 1), (59, 2), (61, 4), (69, 1), (75, 6), (83, 1), (85, 2), (98, 7), (99, 1), (105, 2), (114, 1), (117, 5), (118, 3), (123, 2), (139, 11), (163, 1), (166, 1), (170, 1), (175, 10), (198, 7), (199, 4), (223, 2), (226, 2), (239, 1), (244, 2), (251, 1), (252, 3), (254, 15), (255, 6), (256, 1), (259, 3), (260, 4), (268, 1), (278, 1), (284, 2), (296, 1), (323, 1), (324, 1), (342, 2), (345, 1), (347, 1), (351, 5), (352, 29), (354, 2), (358, 8), (365, 20), (368, 1), (375, 4), (378, 1), (388, 1), (390, 1), (393, 1), (398, 4), (400, 2), (401, 2), (404, 3), (406, 3), (422, 1), (424, 1), (436, 7), (437, 1), (438, 4), (450, 13), (455, 1), (459, 1), (465, 1), (479, 1), (482, 2), (491, 1), (493, 1), (517, 1), (526, 1), (527, 2), (542, 1), (543, 1), (544, 1), (552, 1), (562, 4), (568, 2), (585, 2), (586, 2), (592, 2), (610, 1), (611, 2), (633, 1), (643, 1), (649, 1), (650, 2), (651, 1), (655, 2), (660, 1), (671, 2), (679, 4), (692, 3), (695, 1), (711, 1), (720, 1), (723, 1), (726, 1), (735, 1), (737, 1), (791, 1), (800, 1), (810, 1), (811, 1), (816, 1), (826, 1), (827, 1), (841, 1), (856, 3), (862, 1), (878, 1), (879, 1), (899, 4), (972, 1), (989, 1), (1009, 1), (1075, 1), (1079, 1), (1081, 4), (1082, 1), (1086, 1), (1091, 3), (1093, 1), (1126, 1), (1127, 1), (1151, 2), (1160, 1), (1178, 1), (1180, 1), (1181, 2), (1182, 2), (1183, 3), (1184, 5), (1185, 2), (1186, 1), (1187, 1), (1188, 1), (1189, 1), (1190, 1), (1191, 1), (1192, 1), (1193, 4), (1194, 1), (1195, 1), (1196, 2), (1197, 2), (1198, 1), (1199, 1), (1200, 1), (1201, 1), (1202, 1), (1203, 1), (1204, 2), (1205, 1), (1206, 1), (1207, 4), (1208, 1), (1209, 4), (1210, 1), (1211, 2), (1212, 1), (1213, 1), (1214, 1), (1215, 1), (1216, 1), (1217, 1), (1218, 1), (1219, 1), (1220, 1), (1221, 1), (1222, 2), (1223, 1), (1224, 1), (1225, 1), (1226, 6), (1227, 1), (1228, 2), (1229, 1), (1230, 1), (1231, 1), (1232, 1), (1233, 2), (1234, 1), (1235, 1), (1236, 1), (1237, 1), (1238, 1), (1239, 1), (1240, 1), (1241, 1), (1242, 1), (1243, 1), (1244, 1), (1245, 1), (1246, 2), (1247, 3), (1248, 10), (1249, 1), (1250, 1), (1251, 1), (1252, 2), (1253, 1), (1254, 1), (1255, 1), (1256, 1), (1257, 1), (1258, 1), (1259, 1), (1260, 1), (1261, 1), (1262, 1), (1263, 1), (1264, 1), (1265, 1), (1266, 1), (1267, 1), (1268, 1), (1269, 1), (1270, 1), (1271, 1), (1272, 1), (1273, 4), (1274, 3), (1275, 1), (1276, 3), (1277, 11), (1278, 3), (1279, 1), (1280, 1), (1281, 1), (1282, 1), (1283, 1), (1284, 1), (1285, 10), (1286, 3), (1287, 1), (1288, 1), (1289, 1), (1290, 1), (1291, 1), (1292, 2), (1293, 1), (1294, 3), (1295, 1), (1296, 1), (1297, 1), (1298, 2), (1299, 1), (1300, 2), (1301, 1), (1302, 3), (1303, 1), (1304, 3), (1305, 1), (1306, 1), (1307, 2), (1308, 2), (1309, 1), (1310, 1), (1311, 1), (1312, 1), (1313, 1), (1314, 1), (1315, 1), (1316, 1), (1317, 2), (1318, 1), (1319, 1), (1320, 1), (1321, 1), (1322, 4), (1323, 5), (1324, 2), (1325, 1), (1326, 1), (1327, 2), (1328, 1), (1329, 1), (1330, 1), (1331, 1), (1332, 1), (1333, 1), (1334, 1), (1335, 1), (1336, 1), (1337, 1), (1338, 1), (1339, 1), (1340, 1), (1341, 3), (1342, 1), (1343, 1), (1344, 1), (1345, 1), (1346, 1), (1347, 1), (1348, 1), (1349, 2)]]\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfModel(num_docs=5, num_nnz=1970)\n"
     ]
    }
   ],
   "source": [
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "print(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "fasttext_model300 = api.load('fasttext-wiki-news-subwords-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\ads9ldn\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity_matrix` (Method will be removed in 4.0.0, use gensim.models.keyedvectors.WordEmbeddingSimilarityIndex instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "similarity_matrix = fasttext_model300.similarity_matrix(dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\ads9ldn\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `softcossim` (Function will be removed in 4.0.0, use gensim.similarities.termsim.SparseTermSimilarityMatrix.inner_product instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9548263841157888"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.matutils import softcossim\n",
    "softcossim(corpus[0], corpus[1],similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\ads9ldn\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `softcossim` (Function will be removed in 4.0.0, use gensim.similarities.termsim.SparseTermSimilarityMatrix.inner_product instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9038845756420927"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.matutils import softcossim\n",
    "softcossim(corpus[0], corpus[3],similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\ads9ldn\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `softcossim` (Function will be removed in 4.0.0, use gensim.similarities.termsim.SparseTermSimilarityMatrix.inner_product instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.matutils import softcossim\n",
    "softcossim(corpus[0], corpus[0],similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
